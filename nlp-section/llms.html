<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.19">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Transition to Large Language Models – Machine Learning and Arfificial Intelligence for Medical Professionals</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../nlp-section/word-embeddings.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0af8918f97543ada976083604447deab.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-e9df88e65c6634bd86cecd8b4fc5ed5a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../nlp-section/word-embeddings.html">Natural Language Processing</a></li><li class="breadcrumb-item"><a href="../nlp-section/llms.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Transition to Large Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning and Arfificial Intelligence for Medical Professionals</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../history-of-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">The Evolution of Artificial Intelligence</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../ml-overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview of Machine Learning in Medicine</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ml-overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview of Machine Learning in Medicine</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Natural Language Processing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../nlp-section/word-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Word Embeddings and Representation in Text</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../nlp-section/llms.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Transition to Large Language Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-from-word-embeddings-to-context-aware-models" id="toc-introduction-from-word-embeddings-to-context-aware-models" class="nav-link active" data-scroll-target="#introduction-from-word-embeddings-to-context-aware-models"><span class="header-section-number">4.1</span> Introduction: From Word Embeddings to Context-Aware Models</a></li>
  <li><a href="#transformers-the-foundation-of-large-language-models" id="toc-transformers-the-foundation-of-large-language-models" class="nav-link" data-scroll-target="#transformers-the-foundation-of-large-language-models"><span class="header-section-number">4.2</span> Transformers: The Foundation of Large Language Models</a></li>
  <li><a href="#scaling-up-large-language-models-llms-like-gpt-and-bert" id="toc-scaling-up-large-language-models-llms-like-gpt-and-bert" class="nav-link" data-scroll-target="#scaling-up-large-language-models-llms-like-gpt-and-bert"><span class="header-section-number">4.3</span> Scaling Up: Large Language Models (LLMs) like GPT and BERT</a>
  <ul class="collapse">
  <li><a href="#pre-training-learning-general-knowledge-from-large-datasets" id="toc-pre-training-learning-general-knowledge-from-large-datasets" class="nav-link" data-scroll-target="#pre-training-learning-general-knowledge-from-large-datasets"><span class="header-section-number">4.3.1</span> Pre-training: Learning General Knowledge from Large Datasets</a></li>
  <li><a href="#fine-tuning-specializing-for-specific-tasks" id="toc-fine-tuning-specializing-for-specific-tasks" class="nav-link" data-scroll-target="#fine-tuning-specializing-for-specific-tasks"><span class="header-section-number">4.3.2</span> Fine-tuning: Specializing for Specific Tasks</a></li>
  <li><a href="#instruction-tuning-adapting-to-follow-human-instructions" id="toc-instruction-tuning-adapting-to-follow-human-instructions" class="nav-link" data-scroll-target="#instruction-tuning-adapting-to-follow-human-instructions"><span class="header-section-number">4.3.3</span> Instruction Tuning: Adapting to Follow Human Instructions</a></li>
  <li><a href="#the-role-of-transfer-learning-the-key-underlying-principle" id="toc-the-role-of-transfer-learning-the-key-underlying-principle" class="nav-link" data-scroll-target="#the-role-of-transfer-learning-the-key-underlying-principle"><span class="header-section-number">4.3.4</span> The Role of Transfer Learning: The Key Underlying Principle</a></li>
  </ul></li>
  <li><a href="#llms-in-the-wild" id="toc-llms-in-the-wild" class="nav-link" data-scroll-target="#llms-in-the-wild"><span class="header-section-number">4.4</span> LLMs in the wild</a></li>
  <li><a href="#applications-of-llms-in-healthcare-a-revolution-in-nlp" id="toc-applications-of-llms-in-healthcare-a-revolution-in-nlp" class="nav-link" data-scroll-target="#applications-of-llms-in-healthcare-a-revolution-in-nlp"><span class="header-section-number">4.5</span> Applications of LLMs in Healthcare: A Revolution in NLP</a></li>
  <li><a href="#an-aside-foundation-models-as-the-base-for-diverse-applications" id="toc-an-aside-foundation-models-as-the-base-for-diverse-applications" class="nav-link" data-scroll-target="#an-aside-foundation-models-as-the-base-for-diverse-applications"><span class="header-section-number">4.6</span> An aside: Foundation Models as the Base for Diverse Applications</a>
  <ul class="collapse">
  <li><a href="#key-characteristics-of-foundation-models" id="toc-key-characteristics-of-foundation-models" class="nav-link" data-scroll-target="#key-characteristics-of-foundation-models"><span class="header-section-number">4.6.1</span> Key Characteristics of Foundation Models:</a></li>
  <li><a href="#why-the-term-foundation-model-is-important" id="toc-why-the-term-foundation-model-is-important" class="nav-link" data-scroll-target="#why-the-term-foundation-model-is-important"><span class="header-section-number">4.6.2</span> Why the Term “Foundation Model” Is Important</a></li>
  <li><a href="#example-gpt-as-a-foundation-model" id="toc-example-gpt-as-a-foundation-model" class="nav-link" data-scroll-target="#example-gpt-as-a-foundation-model"><span class="header-section-number">4.6.3</span> Example: GPT as a Foundation Model</a></li>
  <li><a href="#challenges-and-considerations" id="toc-challenges-and-considerations" class="nav-link" data-scroll-target="#challenges-and-considerations"><span class="header-section-number">4.6.4</span> Challenges and Considerations</a></li>
  <li><a href="#in-summary" id="toc-in-summary" class="nav-link" data-scroll-target="#in-summary"><span class="header-section-number">4.6.5</span> In Summary</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">4.7</span> Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../nlp-section/word-embeddings.html">Natural Language Processing</a></li><li class="breadcrumb-item"><a href="../nlp-section/llms.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Transition to Large Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Transition to Large Language Models</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source" data-quarto-source-url="repo"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Natural Language Processing (NLP) has advanced remarkably in recent years, driven by a transition from traditional approaches like word embeddings to large language models (LLMs) based on transformer architectures. To understand this leap, it is crucial to first grasp the limitations of earlier methods and how LLMs overcome them.</p>
<section id="introduction-from-word-embeddings-to-context-aware-models" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction-from-word-embeddings-to-context-aware-models"><span class="header-section-number">4.1</span> Introduction: From Word Embeddings to Context-Aware Models</h2>
<p>Word embeddings, such as Word2Vec and GloVe, marked a significant step forward in NLP by representing words as continuous vectors in a high-dimensional space. These embeddings allowed models to capture semantic relationships between words based on their co-occurrence in large corpora. For instance, the famous relationship between “king” and “queen” being similar to “man” and “woman” <a href="word-embeddings.html#fig-king-queen-analogy" class="quarto-xref">Figure&nbsp;<span>3.3</span></a> became a canonical example of how word vectors capture analogies.</p>
<p>However, static word embeddings like Word2Vec have a key limitation: <em>they assign each word a single vector, regardless of context</em>. For example, the word “bank” will have the same vector whether we are talking about a financial institution or the side of a river. This one-size-fits-all representation struggles with polysemy (multiple meanings of a word) and fails to incorporate the <em>dynamic</em> context in which a word appears.</p>
<p>This limitation set the stage for context-aware models—first through innovations like ELMo (Embeddings from Language Models), which introduced context-dependent embeddings, and later through the transformer architecture, which powers today’s LLMs.</p>
</section>
<section id="transformers-the-foundation-of-large-language-models" class="level2 page-columns page-full" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="transformers-the-foundation-of-large-language-models"><span class="header-section-number">4.2</span> Transformers: The Foundation of Large Language Models</h2>
<p>The transformer architecture, introduced by <span class="citation" data-cites="vaswaniAttentionAllYou2017">(<a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>, represents a paradigm shift in NLP. Unlike earlier recurrent neural networks (RNNs), transformers do not process data sequentially. Instead, they operate on entire sequences of words (or tokens) at once, making them highly parallelizable and more efficient for training on large datasets.</p>
<p>At the heart of transformers is the attention mechanism. Attention allows the model to focus on relevant parts of the input sequence while processing a word. This capability enables transformers to capture long-range dependencies and relationships between words more effectively than RNNs or convolutional neural networks (CNNs).</p>
<p>The input to a transformer is typically a sequence of word embeddings, which are passed through multiple layers of encoders. Each encoder consists of two main components: a self-attention mechanism and a feed-forward neural network. The self-attention mechanism helps the model weigh the importance of different words in the sequence, while the feed-forward network processes this information to generate the final output. Remember that one of the shortcomings of word embeddings is that they do not capture context; each word has one-and-only-one embedding vector. Transformers address this by considering the entire sequence when encoding a word.</p>
<p><a href="#fig-attention-mechanism" class="quarto-xref">Figure&nbsp;<span>4.1</span></a> shows how the attention mechanism helps the model understand the context of a word in a sentence. The encoding of the word “it” in the sentence “The animal didn’t cross the street because it was too tired” is influenced by the attention mechanism, which focuses on “The Animal” to understand the referent of “it”.</p>
<div id="fig-attention-mechanism" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-attention-mechanism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;4.1: Attention: As we are encoding the word “it” in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on “The Animal”, and baked a part of its representation into the encoding of “it”."><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-attention-mechanism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Attention: As we are encoding the word “it” in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on “The Animal”, and baked a part of its representation into the encoding of “it”.
</figcaption>
</figure>
</div>
<p><strong>Figure 2: Example of attention mechanism highlighting key word dependencies within a sentence.</strong></p>
<p>The architecture of transformers consists of an encoder and a decoder. While the original transformer model was designed for sequence-to-sequence tasks like translation, recent advancements focus on pre-training the encoder (in models like BERT) or both encoder and decoder (in models like GPT). These models, known as large language models, can be fine-tuned for various downstream tasks, including text generation, question answering, and summarization.</p>
<p>Again, one of the key innovations in transformers is the concept of <em>self-attention</em>, which allows the model to weigh the importance of different words in a sentence when encoding a particular word. In transformers, this process is done in parallel for multiple attention heads, leading to the concept of <em>multi-head attention</em>. Multi-head attention enables the model to focus on different aspects of a sentence simultaneously. While one head might focus on syntactic structure (such as subject-verb agreement), another might capture semantic relationships (like identifying the recipient of an action).</p>
<p>Table 1 below summarizes the advantages of self-attention and multi-head attention compared to previous methods like RNNs and CNNs:</p>
<p><strong>Table 1: Advantages of Self-Attention and Multi-Head Attention</strong></p>
<table>
<colgroup>
<col style="width: 30%">
<col style="width: 30%">
<col style="width: 38%">
</colgroup>
<thead>
<tr>
<th>Feature</th>
<th>RNNs / CNNs</th>
<th>Transformers (Self-Attention)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sequential Processing</strong></td>
<td>Yes (slower, non-parallel)</td>
<td>No (parallel processing of entire input)</td>
</tr>
<tr>
<td><strong>Capturing Long-Range Dependencies</strong></td>
<td>Limited (vanishing gradients)</td>
<td>Efficiently captures distant relationships</td>
</tr>
<tr>
<td><strong>Contextual Understanding</strong></td>
<td>Limited to fixed window</td>
<td>Full sequence considered for each word</td>
</tr>
<tr>
<td><strong>Training Efficiency</strong></td>
<td>Slower due to sequence dependencies</td>
<td>Highly efficient (leverages parallelism)</td>
</tr>
</tbody>
</table>
</section>
<section id="scaling-up-large-language-models-llms-like-gpt-and-bert" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="scaling-up-large-language-models-llms-like-gpt-and-bert"><span class="header-section-number">4.3</span> Scaling Up: Large Language Models (LLMs) like GPT and BERT</h2>
<p>Transformers provided the foundation for large language models (LLMs), but what distinguishes LLMs from smaller models is their sheer size. LLMs such as OpenAI’s GPT (Generative Pre-trained Transformer) and Google’s BERT (Bidirectional Encoder Representations from Transformers) are trained on massive corpora of text, often with billions or even trillions of parameters. This enables them to generalize across a wide variety of NLP tasks.</p>
<p>GPT models, for example, are designed for text generation tasks, while BERT models are pre-trained for bidirectional understanding of language. BERT’s bidirectional nature allows it to capture context from both left and right contexts, making it particularly effective for tasks like question answering and text classification.</p>
<p>When discussing the process of building and refining large language models (LLMs), it’s crucial to understand the progression from <strong>pre-training</strong> to <strong>fine-tuning</strong> and <strong>instruction tuning</strong>. These phases build upon each other, and all rely on the powerful principle of <strong>transfer learning</strong>.</p>
<section id="pre-training-learning-general-knowledge-from-large-datasets" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="pre-training-learning-general-knowledge-from-large-datasets"><span class="header-section-number">4.3.1</span> Pre-training: Learning General Knowledge from Large Datasets</h3>
<p>The first step in developing a large language model is <strong>pre-training</strong>. In this phase, the model is trained on vast amounts of diverse text data from the internet—news articles, books, websites, social media posts, etc. The goal is for the model to learn general patterns in language: grammar, sentence structure, and relationships between words.</p>
<p>During pre-training, the model is not explicitly trained to perform any specific task (such as answering questions or translating text). Instead, it learns by predicting the next word in a sentence (a common objective known as <em>masked language modeling</em> or <em>causal language modeling</em>). This helps the model build a broad, high-level understanding of how language works.</p>
<ul>
<li><strong>Example</strong>: If the model is given the sentence, “The cat is sitting on the ___,” it learns to predict that “mat” or “sofa” might be appropriate completions based on what it has seen in similar contexts during training.</li>
</ul>
<p>By the end of this phase, the model has developed a deep, though general, understanding of language, but it hasn’t yet been specialized for any particular task.</p>
</section>
<section id="fine-tuning-specializing-for-specific-tasks" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="fine-tuning-specializing-for-specific-tasks"><span class="header-section-number">4.3.2</span> Fine-tuning: Specializing for Specific Tasks</h3>
<p>The <strong>fine-tuning</strong> phase builds on the knowledge gained during pre-training. Here, the model is trained on smaller, task-specific datasets to specialize in a particular task, such as medical diagnosis, summarization, or chatbot responses.</p>
<p>In this stage, transfer learning is applied. The general language understanding developed during pre-training is <strong>transferred</strong> to the new task, allowing the model to adapt quickly with much less data than would otherwise be required. Fine-tuning modifies the model slightly to better fit the specific needs of the task, but it doesn’t start from scratch. Instead, the model reuses and adapts what it already knows.</p>
<ul>
<li><strong>Example</strong>: Suppose a pre-trained model is fine-tuned for medical language. During fine-tuning, it adapts its broad language knowledge to better understand medical terminology, patient reports, and clinical guidelines. This allows the model to assist healthcare professionals more effectively in tasks like summarizing patient history or answering medical queries.</li>
</ul>
<p>The use of <strong>transfer learning</strong> means that the model can generalize well from smaller datasets, because it leverages the vast amount of language knowledge it has already acquired during pre-training.</p>
</section>
<section id="instruction-tuning-adapting-to-follow-human-instructions" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="instruction-tuning-adapting-to-follow-human-instructions"><span class="header-section-number">4.3.3</span> Instruction Tuning: Adapting to Follow Human Instructions</h3>
<p><strong>Instruction tuning</strong> is a refinement of fine-tuning where the model is trained to follow structured, task-specific human instructions. In this phase, the model is provided with examples of how humans give instructions and what the desired outcomes should look like.</p>
<p>Transfer learning plays a role here too—again, the model is building upon the language patterns learned during pre-training, and it adapts those patterns to better understand and respond to human commands.</p>
<ul>
<li><strong>Example</strong>: A model might be given a prompt like, “Summarize this article in two sentences” or “Translate this sentence to French.” During instruction tuning, it learns how to execute these specific requests more effectively.</li>
</ul>
<p>By the end of instruction tuning, the model becomes more user-friendly and better at handling structured, goal-oriented tasks. This is the stage where models begin to behave more like assistants that can respond coherently to specific requests from users.</p>
</section>
<section id="the-role-of-transfer-learning-the-key-underlying-principle" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="the-role-of-transfer-learning-the-key-underlying-principle"><span class="header-section-number">4.3.4</span> The Role of Transfer Learning: The Key Underlying Principle</h3>
<p><strong>Transfer learning</strong> is the foundation that makes the transition from pre-training to fine-tuning and instruction tuning possible. It enables the model to <strong>transfer</strong> its general knowledge from the pre-training phase to the more specialized tasks it encounters during fine-tuning and instruction tuning. This is why transfer learning is considered a key advantage of LLMs—without it, each task would require training a model from scratch, which would be computationally expensive and data-intensive.</p>
<p>In simpler terms, transfer learning is the reason why a model trained on large, general datasets can still perform well on specific tasks with relatively small amounts of data. It’s not a separate step but rather a fundamental property that makes fine-tuning and instruction tuning both efficient and effective.</p>
<p>This version emphasizes the flow of how transfer learning operates across the stages, clarifying that it is the mechanism enabling these transitions, rather than a separate part of the model-building process. Let me know if this works for you or if you’d like further adjustments!</p>
</section>
</section>
<section id="llms-in-the-wild" class="level2 page-columns page-full" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="llms-in-the-wild"><span class="header-section-number">4.4</span> LLMs in the wild</h2>
<p>While most folks are familiar with ChatGPT, there are many other LLMs that are available for us (See <a href="#tbl-llm-models" class="quarto-xref">Table&nbsp;<span>4.1</span></a>). Many of the commercially available LLMs have a “free-tier” that allows for everyday use. In addition to very large models like ChatGPT, there are smaller models that, in many cases, are more than sufficient for many tasks, are more cost-effective, are faster to run, and are more energy-efficient.</p>
<div id="tbl-llm-models" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full">
<div aria-describedby="tbl-llm-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table>
<thead>
<tr>
<th>Model Name</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td><a href="https://chat.openai.com/">Link</a></td>
</tr>
<tr>
<td>Claude</td>
<td><a href="https://claude.ai/">Link</a></td>
</tr>
<tr>
<td>Gemini</td>
<td><a href="https://gemini.google.com/app">Link</a></td>
</tr>
<tr>
<td>HuggingChat</td>
<td><a href="https://huggingface.co/chat">Link</a></td>
</tr>
<tr>
<td>Microsoft Copilot</td>
<td><a href="https://copilot.cloud.microsoft/">Link</a></td>
</tr>
<tr>
<td>groq</td>
<td><a href="https://groq.com/">Link</a></td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-llm-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.1: A selection of large language models (LLMs) available for use, including ChatGPT, Claude, Gemini, HuggingChat, Microsoft Copilot, and groq.
</figcaption>
</figure>
</div>
</section>
<section id="applications-of-llms-in-healthcare-a-revolution-in-nlp" class="level2 page-columns page-full" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="applications-of-llms-in-healthcare-a-revolution-in-nlp"><span class="header-section-number">4.5</span> Applications of LLMs in Healthcare: A Revolution in NLP</h2>
<p>This section is structured as an abbreviated literature review, highlighting key studies and findings in the field of healthcare NLP. It showcases the transformative impact of large language models (LLMs) like GPT and BERT on various healthcare applications, including clinical documentation, medical imaging, and patient care.</p>
<p>In this short review by <span class="citation" data-cites="shahCreationAdoptionLarge2023">Shah, Entwistle, and Pfeffer (<a href="#ref-shahCreationAdoptionLarge2023" role="doc-biblioref">2023</a>)</span>, the authors observe that LLMs are being adopted in healthcare, but often without proper evaluation and focus on the goals and the extent to which LLMs reach them. They propose that instead of asking “New users have been asking how the LLMs and the chatbots powered by them will reshape medicine,” we should be asking “How can the intended medical use shape the training of the LLMs and the chatbots or the other applications they power?”</p>
<p>In what is now considered a seminal paper, <span class="citation" data-cites="ayersComparingPhysicianArtificial2023">Ayers et al. (<a href="#ref-ayersComparingPhysicianArtificial2023" role="doc-biblioref">2023</a>)</span> compared the responses of physicians and AI chatbots to patient questions sourced from Reddit’s r/AskDocs. <a href="#tbl-chatbot-physician-comparison" class="quarto-xref">Table&nbsp;<span>4.2</span></a> summarizes the key findings of this cross-sectional study, highlighting the advantages of chatbot responses in terms of length, quality of information, and empathy.</p>
<div id="tbl-chatbot-physician-comparison" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full">
<div aria-describedby="tbl-chatbot-physician-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 26%">
<col style="width: 19%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;"><strong>Measure</strong></th>
<th style="text-align: left;"><strong>Physicians</strong></th>
<th style="text-align: left;"><strong>Chatbot</strong></th>
<th style="text-align: left;"><strong>Statistical Significance</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Evaluator Preference</strong></td>
<td style="text-align: left;">Preferred in 21.4% (95% CI, 18.2%-25.0%)</td>
<td style="text-align: left;">Preferred in 78.6% (95% CI, 75.0%-81.8%)</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Response Length (mean words)</strong></td>
<td style="text-align: left;">52 (IQR 17-62)</td>
<td style="text-align: left;">211 (IQR 168-245)</td>
<td style="text-align: left;">t = 25.4, P &lt; .001</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Quality of Information (mean score)</strong></td>
<td style="text-align: left;">Significantly lower (22.1% rated as good or very good, 95% CI, 16.4%-28.2%)</td>
<td style="text-align: left;">Significantly higher (78.5% rated as good or very good, 95% CI, 72.3%-84.1%)</td>
<td style="text-align: left;">t = 13.3, P &lt; .001</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Empathy of Responses (mean score)</strong></td>
<td style="text-align: left;">4.6% rated empathetic or very empathetic (95% CI, 2.1%-7.7%)</td>
<td style="text-align: left;">45.1% rated empathetic or very empathetic (95% CI, 38.5%-51.8%)</td>
<td style="text-align: left;">t = 18.9, P &lt; .001</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-chatbot-physician-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.2: Summary of findings from a cross-sectional study comparing chatbot (ChatGPT) and physician responses to patient questions sourced from Reddit’s r/AskDocs. Evaluators significantly favored chatbot responses, which were longer, rated as higher quality, and more empathetic. This suggests AI chatbots may be useful for drafting patient responses, potentially reducing clinician workload and improving patient engagement. Future research, including randomized trials, is warranted to explore AI’s role in enhancing clinical care and reducing physician burnout.
</figcaption>
</figure>
</div>
<p>An extensive review of research about and use of LLMs in healthcare by <span class="citation" data-cites="luLargeLanguageModels2024">Lu et al. (<a href="#ref-luLargeLanguageModels2024" role="doc-biblioref">2024</a>)</span> provides a comprehensive summary of the Journal of the American Medical Informatics Association (JAMIA) special issue on LLMs in healthcare. The review covers a wide range of topics, including the use of LLMs in clinical documentation, medical imaging, patient care, and more. <a href="#fig-llms-in-jamia-special-issue" class="quarto-xref">Figure&nbsp;<span>4.2</span></a> visualizes the various LLMs discussed in the special issue.</p>
<div id="fig-llms-in-jamia-special-issue" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-llms-in-jamia-special-issue-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="llms-in-jamia-special-issue.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;4.2: LLMs used in the special issue of JAMIA [^]"><img src="llms-in-jamia-special-issue.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-llms-in-jamia-special-issue-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: LLMs used in the special issue of JAMIA [^]
</figcaption>
</figure>
</div>
<p>Integrating LLMs into healthcare settings has a lot of potential, but doing so is hard. <span class="citation" data-cites="labkoffResponsibleFutureRecommendations2024">Labkoff et al. (<a href="#ref-labkoffResponsibleFutureRecommendations2024" role="doc-biblioref">2024</a>)</span> “aims to make practical suggestions for creating methods, rules, and guidelines to ensure that the development, testing, supervision, and use of AI in clinical decision support (CDS) systems are done well and safely for patients.” The authors suggest four key recommendations for doing so <span class="citation" data-cites="labkoffResponsibleFutureRecommendations2024">(<a href="#ref-labkoffResponsibleFutureRecommendations2024" role="doc-biblioref">Labkoff et al. 2024</a>)</span>.</p>
<ol type="1">
<li>Building safe and trustworthy systems;</li>
<li>Developing validation, verification, and certification processes for AI-CDS systems;</li>
<li>Providing a means of safety monitoring and reporting at the national level; and</li>
<li>Ensuring that appropriate documentation and end-user training are provided.</li>
</ol>
<p>The paper is packed with practical advice and insights for healthcare professionals and AI developers looking to integrate LLMs into clinical practice.</p>
<p>The status of public comfort with the use of ChatGPT in healthcare was evaluated by <span class="citation" data-cites="plattPublicComfortUse2024a">Platt et al. (<a href="#ref-plattPublicComfortUse2024a" role="doc-biblioref">2024</a>)</span>.</p>
<p>A group of investigators has been collaborating to produce and inroduce a CLinical Artificial Intelligence Checklist (MI-CLAIM-GEN) for reporting information about a generative model <span class="citation" data-cites="miaoMinimumInformationCLinical2024">(<a href="#ref-miaoMinimumInformationCLinical2024" role="doc-biblioref">Miao et al. 2024</a>)</span>. The checklist is designed to help developers and users of generative models understand the model’s development, intended use, performance, limitations, and recommendations for safe deployment. The checklist is intended to improve transparency and trust in the use of generative models in clinical settings. An example of a suggested clinical model card is shown in <a href="#fig-clinical-model-card" class="quarto-xref">Figure&nbsp;<span>4.3</span></a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-clinical-model-card" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clinical-model-card-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="clinical-model-card.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;4.3: An example model card as proposed by @miaoMinimumInformationCLinical2024, formatted as a clinical “model facts” label, for a fictional model created to assist in clinical decision support around sepsis diagnosis and management. The clinical model card should provide a summary of how a model was developed, intended use, out-of-scope uses, performance, limitations, and recommendations for safe deployment."><img src="clinical-model-card.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig" id="fig-clinical-model-card-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: An example model card as proposed by <span class="citation" data-cites="miaoMinimumInformationCLinical2024">Miao et al. (<a href="#ref-miaoMinimumInformationCLinical2024" role="doc-biblioref">2024</a>)</span>, formatted as a clinical “model facts” label, for a fictional model created to assist in clinical decision support around sepsis diagnosis and management. The clinical model card should provide a summary of how a model was developed, intended use, out-of-scope uses, performance, limitations, and recommendations for safe deployment.
</figcaption>
</figure>
</div>
</div></div><p>Numerous collections of high-quality, highly-cited work that includes LLMs in healthcare and biomedical research are available and highlighted in <a href="#tbl-llm-healthcare-collections" class="quarto-xref">Table&nbsp;<span>4.3</span></a>.</p>
<div id="tbl-llm-healthcare-collections" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full">
<div aria-describedby="tbl-llm-healthcare-collections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table>
<thead>
<tr>
<th style="text-align: left;">Collection</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="https://academic.oup.com/jamia/pages/llm-special-issue">JAMIA Special Issue</a></td>
<td style="text-align: left;">A collection of research articles on LLMs in healthcare</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://jamanetwork.com/collections/44024/artificial-intelligence">JAMA Network AI Collection</a></td>
<td style="text-align: left;">A series of articles on AI in healthcare from JAMA Network</td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://www.nature.com/collections/dbfcjjigbi">Nature AI Collection</a></td>
<td style="text-align: left;">A curated selection of AI research in medicine from Nature Portfolio</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-llm-healthcare-collections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.3: Collections of research articles on AI and LLMs in healthcare from leading journals and publishers.
</figcaption>
</figure>
</div>
</section>
<section id="an-aside-foundation-models-as-the-base-for-diverse-applications" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="an-aside-foundation-models-as-the-base-for-diverse-applications"><span class="header-section-number">4.6</span> An aside: Foundation Models as the Base for Diverse Applications</h2>
<p>A <strong>foundation model</strong> refers to a large, pre-trained model that serves as a general-purpose starting point for a wide range of downstream tasks. These models are typically trained on massive datasets using self-supervised learning techniques, allowing them to capture a broad and versatile understanding of the data, such as language or images. Once trained, foundation models can be fine-tuned or adapted for specific tasks, like translation, summarization, or image recognition, through additional, smaller-scale training phases.</p>
<p>The term is often used in the context of large-scale machine learning models like <strong>GPT (Generative Pre-trained Transformers)</strong> and <strong>BERT (Bidirectional Encoder Representations from Transformers)</strong> in natural language processing (NLP), or models like <strong>CLIP</strong> and <strong>DALL-E</strong> for vision tasks. These models can be thought of as foundational because they provide a <strong>base</strong> of knowledge that can be reused and customized for numerous applications.</p>
<section id="key-characteristics-of-foundation-models" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="key-characteristics-of-foundation-models"><span class="header-section-number">4.6.1</span> Key Characteristics of Foundation Models:</h3>
<ol type="1">
<li><p><strong>Pre-trained on Massive Datasets</strong>: Foundation models are typically pre-trained on vast amounts of unlabeled data, such as text scraped from the web or millions of images, enabling them to learn patterns and representations that generalize across domains.</p></li>
<li><p><strong>Self-supervised Learning</strong>: Pre-training often uses self-supervised learning techniques, where the model learns to predict parts of the input (e.g., predicting the next word in a sentence) without requiring labeled data. This enables the model to scale to very large datasets.</p></li>
<li><p><strong>Versatility</strong>: Because they are trained to capture general patterns in the data, foundation models can be fine-tuned or adapted for a wide range of tasks with relatively little task-specific data. For example, a foundation language model can be fine-tuned for tasks like question answering, summarization, or even medical diagnosis with only a modest amount of new data for each specific task.</p></li>
<li><p><strong>Transfer Learning</strong>: Foundation models are excellent at leveraging <strong>transfer learning</strong>, meaning that the knowledge they acquire during pre-training is transferable to new tasks with minimal additional training. This reduces the need for training a new model from scratch for each task.</p></li>
<li><p><strong>Scalability</strong>: The term “foundation” highlights how these models serve as a scalable base for many different applications. Once the initial model is pre-trained, it can be fine-tuned for different domains, such as legal text, scientific literature, or medical data, without needing to retrain the entire model from scratch.</p></li>
<li><p><strong>Generalization across Modalities</strong>: Some foundation models, such as <strong>CLIP</strong>, operate across different data types (e.g., images and text), allowing them to generalize across modalities, making them highly adaptable for diverse use cases.</p></li>
</ol>
</section>
<section id="why-the-term-foundation-model-is-important" class="level3" data-number="4.6.2">
<h3 data-number="4.6.2" class="anchored" data-anchor-id="why-the-term-foundation-model-is-important"><span class="header-section-number">4.6.2</span> Why the Term “Foundation Model” Is Important</h3>
<p>The significance of <strong>foundation models</strong> lies in their ability to democratize machine learning development. Instead of requiring each new application to build a model from scratch, developers can use these pre-trained models as a starting point, saving time, computational resources, and data. This shift allows for the rapid deployment of models in a wide variety of fields, from healthcare to finance to creative industries.</p>
<p>For example: - <strong>GPT-3</strong>, a foundation model for natural language processing, can be adapted to answer questions, write essays, or even generate programming code. - <strong>BERT</strong> has been used as the foundation for various NLP tasks, such as named entity recognition, text classification, and sentiment analysis. - <strong>CLIP</strong> combines text and images to provide capabilities such as image-text matching, zero-shot classification, and creative image generation.</p>
</section>
<section id="example-gpt-as-a-foundation-model" class="level3" data-number="4.6.3">
<h3 data-number="4.6.3" class="anchored" data-anchor-id="example-gpt-as-a-foundation-model"><span class="header-section-number">4.6.3</span> Example: GPT as a Foundation Model</h3>
<p>Consider <strong>GPT-3</strong>, one of the most well-known foundation models in NLP. GPT-3 was trained on a diverse range of text data from the internet, giving it a broad understanding of language patterns and general knowledge. After pre-training, GPT-3 can be fine-tuned or adapted to various tasks—whether it’s answering customer support questions, translating text between languages, or generating creative content. The key is that GPT-3 doesn’t need to be retrained from scratch for every new task; its “foundational” understanding of language can be reused and adapted.</p>
</section>
<section id="challenges-and-considerations" class="level3" data-number="4.6.4">
<h3 data-number="4.6.4" class="anchored" data-anchor-id="challenges-and-considerations"><span class="header-section-number">4.6.4</span> Challenges and Considerations</h3>
<p>While foundation models have opened up numerous possibilities, there are also challenges associated with them: - <strong>Bias</strong>: Since foundation models are trained on vast and uncurated datasets, they often inherit biases present in the data (e.g., gender, racial, or societal biases). Fine-tuning can help reduce these biases, but they remain a significant concern. - <strong>Compute and Energy Costs</strong>: Pre-training foundation models requires enormous computational resources, and the environmental impact of training such large models is a growing area of concern. - <strong>Overfitting to the Internet’s Text</strong>: Pre-trained foundation models may overfit to the types of data they were exposed to during training (often text from the internet) and may struggle with highly specialized domains without substantial fine-tuning.</p>
</section>
<section id="in-summary" class="level3" data-number="4.6.5">
<h3 data-number="4.6.5" class="anchored" data-anchor-id="in-summary"><span class="header-section-number">4.6.5</span> In Summary</h3>
<p>A <strong>foundation model</strong> is a large, pre-trained machine learning model that serves as a versatile base for a wide variety of downstream tasks. It leverages transfer learning to apply the broad patterns it learned during pre-training to specific tasks with relatively little additional data. While they offer powerful capabilities, foundation models also pose challenges related to bias, resource requirements, and task-specific adaptation.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">4.7</span> Conclusion</h2>
<p>The transition to large language models (LLMs) based on transformer architectures has significantly advanced the field of natural language processing. By moving beyond static word embeddings to context-aware models, LLMs like GPT and BERT have demonstrated remarkable capabilities in understanding and generating human language. These models, powered by transfer learning and self-attention mechanisms, have found diverse applications across industries, including healthcare, finance, and creative fields.</p>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ayersComparingPhysicianArtificial2023" class="csl-entry" role="listitem">
Ayers, John W., Adam Poliak, Mark Dredze, Eric C. Leas, Zechariah Zhu, Jessica B. Kelley, Dennis J. Faix, et al. 2023. <span>“Comparing <span>Physician</span> and <span>Artificial</span> <span>Intelligence</span> <span>Chatbot</span> <span>Responses</span> to <span>Patient</span> <span>Questions</span> <span>Posted</span> to a <span>Public</span> <span>Social</span> <span>Media</span> <span>Forum</span>.”</span> <em>JAMA Internal Medicine</em> 183 (6): 589–96. <a href="https://doi.org/10.1001/jamainternmed.2023.1838">https://doi.org/10.1001/jamainternmed.2023.1838</a>.
</div>
<div id="ref-labkoffResponsibleFutureRecommendations2024" class="csl-entry" role="listitem">
Labkoff, Steven, Bilikis Oladimeji, Joseph Kannry, Anthony Solomonides, Russell Leftwich, Eileen Koski, Amanda L Joseph, et al. 2024. <span>“Toward a Responsible Future: Recommendations for <span>AI</span>-Enabled Clinical Decision Support.”</span> <em>Journal of the American Medical Informatics Association</em>, September, ocae209. <a href="https://doi.org/10.1093/jamia/ocae209">https://doi.org/10.1093/jamia/ocae209</a>.
</div>
<div id="ref-luLargeLanguageModels2024" class="csl-entry" role="listitem">
Lu, Zhiyong, Yifan Peng, Trevor Cohen, Marzyeh Ghassemi, Chunhua Weng, and Shubo Tian. 2024. <span>“Large Language Models in Biomedicine and Health: Current Research Landscape and Future Directions.”</span> <em>Journal of the American Medical Informatics Association</em> 31 (9): 1801–11. <a href="https://doi.org/10.1093/jamia/ocae202">https://doi.org/10.1093/jamia/ocae202</a>.
</div>
<div id="ref-miaoMinimumInformationCLinical2024" class="csl-entry" role="listitem">
Miao, Brenda Y., Irene Y. Chen, Christopher YK Williams, Jaysón Davidson, Augusto Garcia-Agundez, Shenghuan Sun, Travis Zack, et al. 2024. <span>“The <span>Minimum</span> <span>Information</span> about <span>CLinical</span> <span>Artificial</span> <span>Intelligence</span> <span>Checklist</span> for <span>Generative</span> <span>Modeling</span> <span>Research</span> (<span>MI</span>-<span>CLAIM</span>-<span>GEN</span>).”</span> arXiv. <a href="http://arxiv.org/abs/2403.02558">http://arxiv.org/abs/2403.02558</a>.
</div>
<div id="ref-plattPublicComfortUse2024a" class="csl-entry" role="listitem">
Platt, Jodyn, Paige Nong, Renée Smiddy, Reema Hamasha, Gloria Carmona Clavijo, Joshua Richardson, and Sharon L R Kardia. 2024. <span>“Public Comfort with the Use of <span>ChatGPT</span> and Expectations for Healthcare.”</span> <em>Journal of the American Medical Informatics Association</em> 31 (9): 1976–82. <a href="https://doi.org/10.1093/jamia/ocae164">https://doi.org/10.1093/jamia/ocae164</a>.
</div>
<div id="ref-shahCreationAdoptionLarge2023" class="csl-entry" role="listitem">
Shah, Nigam H, David Entwistle, and Michael A Pfeffer. 2023. <span>“Creation and <span>Adoption</span> of <span>Large</span> <span>Language</span> <span>Models</span> in <span>Medicine</span>.”</span> <em>JAMA: The Journal of the American Medical Association</em> 330 (9): 866–69. <a href="https://doi.org/10.1001/jama.2023.14217">https://doi.org/10.1001/jama.2023.14217</a>.
</div>
<div id="ref-vaswaniAttentionAllYou2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention <span>Is</span> <span>All</span> <span>You</span> <span>Need</span>.”</span> <em>arXiv [Cs.CL]</em>, June. <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../nlp-section/word-embeddings.html" class="pagination-link" aria-label="Word Embeddings and Representation in Text">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Word Embeddings and Representation in Text</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","selector":".lightbox","openEffect":"zoom","loop":false});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>