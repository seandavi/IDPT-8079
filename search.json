[
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html",
    "href": "Exercises/aom-expert-system/play-with-prolog.html",
    "title": "Play with Prolog",
    "section": "",
    "text": "Understand Expert Systems: Know what an expert system is, particularly as a narrow AI system.\nExplore Logic Programming: Learn the basic principles of logic programming and how Prolog is used to model complex decision-making processes.\nLearn to Implement an Expert System: Gain hands-on experience using programming to solve problems using logical rules and knowledge representation.\nDevelop Problem-Solving Skills: Implement a simple expert system in Prolog, mimicking the decision-making capabilities of a human expert.\nUnderstand limitations of Expert Systems: Explore the concept of narrow AI and its practical applications in specific domains through the use of expert systems."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#components-of-an-expert-system",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#components-of-an-expert-system",
    "title": "Play with Prolog",
    "section": "Components of an Expert System",
    "text": "Components of an Expert System\nAn ES is built around a knowledge base that contains a vast amount of information, rules, and relationships specific to the domain it’s designed for. This knowledge is typically acquired from human experts, research papers, or other sources. The knowledge base is organized and structured to facilitate efficient reasoning and problem-solving. Think of this as the “rules” or “facts” that the system uses to make decisions. In a medical setting, gathering the knowledge base might involve reviewing textbooks, guidelines, and expert opinions to extract the key information needed to make diagnoses and treatment recommendations.\nThe inference engine is the “brain” of the ES. It uses the knowledge base to draw conclusions, make decisions, and solve problems. The engine applies logical rules and reasoning techniques to arrive at a solution. Note that the inference engine doesn’t “learn” in the traditional sense, but rather applies predefined rules to the input data. New rules can be added to the system to expand its capabilities, but the system doesn’t learn from experience like a neural network would.\nAn ES typically has a user-friendly interface that allows users to input queries, ask questions, or provide data. The system then uses this input to generate a response, provide recommendations, or solve a problem. The user interface can be text-based, graphical, or voice-activated, depending on the application.\nESs use various reasoning techniques, such as forward and backward chaining, to solve problems and make decisions. The reasoning and problem solving capabilities of an ES are what set it apart from traditional software systems. Languages like Prolog are commonly used to implement the logic and reasoning components of an ES."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#applications-of-expert-systems",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#applications-of-expert-systems",
    "title": "Play with Prolog",
    "section": "Applications of Expert Systems",
    "text": "Applications of Expert Systems\nExpert Systems have been applied in various domains and industries, including healthcare, finance, manufacturing, and customer service. ESs can help diagnose diseases, recommend treatments, and guide patient care. ESs can monitor production processes, detect defects, and recommend quality improvements. ESs can provide customer support, answer frequently asked questions, and route complex issues to human representatives."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#limitations-of-expert-systems",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#limitations-of-expert-systems",
    "title": "Play with Prolog",
    "section": "Limitations of Expert Systems",
    "text": "Limitations of Expert Systems\nWhile Expert Systems have shown significant promise, they also have some limitations. Building an ES requires a significant amount of knowledge acquisition, which can be time-consuming and costly. The knowledge base must be accurately represented and organized to ensure effective reasoning and problem-solving. ESs require ongoing maintenance to keep their knowledge base up-to-date and ensure they remain effective. ESs may struggle with complex, ambiguous, or uncertain problems that require human intuition or creativity. Because ESs rely on predefined rules and logic, they may not adapt well to new or unexpected situations such as additional symptoms, tests, or rare conditions not accounted for when the system was built."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#using-chatgpt-or-claude-to-understand-expert-systems",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#using-chatgpt-or-claude-to-understand-expert-systems",
    "title": "Play with Prolog",
    "section": "Using ChatGPT (or Claude) to understand Expert Systems",
    "text": "Using ChatGPT (or Claude) to understand Expert Systems\nBefore we dive into the actual exercise, ask ChatGPT (or Claude) a few questions to understand the concept of Expert Systems better. Some examples might include:\n\nWhat is an Expert System?\nHow do Expert Systems work?\nWhat are the components of an Expert System?\nWhat are some applications of Expert Systems?\nWhat are the limitations of Expert Systems?\nHow are Expert Systems different from traditional software systems?\nHow do expert systems compare to machine learning systems?\nWhat are some examples of Expert Systems in healthcare?"
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#key-features-of-prolog",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#key-features-of-prolog",
    "title": "Play with Prolog",
    "section": "Key features of Prolog",
    "text": "Key features of Prolog\n\nDeclarative Approach: Prolog focuses on what needs to be solved rather than how to solve it. Think of it like giving a set of rules and facts, and then letting the computer figure out the solution on its own, rather than telling it step-by-step how to get there.\nLogical Reasoning: Prolog is built on logic. It uses logical thinking to figure things out, much like how we reason through problems in everyday life. For example, if you tell it certain facts like “All humans are mortal” and “Socrates is a human,” Prolog can figure out that “Socrates is mortal.”\nFacts, Rules, and Questions: Prolog programs are made up of facts (like “Socrates is a human”), rules (like “all humans are mortal”), and questions (like “is Socrates mortal?”). You give Prolog the facts and rules, and then you can ask it questions. It works to find the answers using the information you’ve provided.\n\nIn short, Prolog is like a puzzle solver that uses logic and rules to find solutions, and you don’t have to tell it every step—it figures that part out by itself!"
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#role-in-ai",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#role-in-ai",
    "title": "Play with Prolog",
    "section": "Role in AI",
    "text": "Role in AI\nProlog has played a significant role in the history of AI and has been used in various AI applications. Some of the key areas where Prolog has been applied include:\n\nExpert Systems: In the 1980s, Prolog was widely used to develop expert systems, which rely on predefined knowledge and logical rules to provide decision-making capabilities.\nNatural Language Processing (NLP): Prolog’s strengths in pattern matching and symbolic reasoning made it a good choice for early NLP research and applications, as it could model syntactic and semantic relationships in language.\nAutomated Theorem Proving: Due to its foundation in formal logic, Prolog was used in AI systems aimed at proving theorems and solving puzzles."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#historical-significance",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#historical-significance",
    "title": "Play with Prolog",
    "section": "Historical Significance",
    "text": "Historical Significance\nProlog played a crucial role in early AI research. It was one of the key languages, alongside LISP, that shaped the development of AI methodologies, particularly in areas such as knowledge representation, reasoning, and machine learning. In the 1980s, Prolog gained popularity when it was adopted by the Japanese Fifth Generation Computer Systems (FGCS) project, which aimed to develop intelligent computers. Although the project did not meet all of its goals, it contributed to Prolog’s international prominence."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#current-usage",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#current-usage",
    "title": "Play with Prolog",
    "section": "Current Usage",
    "text": "Current Usage\nProlog is not as widely used today as mainstream languages like Python or Java, particularly in AI development. However, it still has a niche role in certain areas of AI, particularly in research and specialized fields like automated reasoning, logic programming, and computational linguistics. Prolog continues to be taught in some academic settings as a tool for understanding logic programming and the fundamentals of AI.\nProlog remains an important historical pillar in AI, particularly for its contributions to logical reasoning, knowledge representation, and expert systems."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#an-example-prolog-program",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#an-example-prolog-program",
    "title": "Play with Prolog",
    "section": "An example Prolog program",
    "text": "An example Prolog program\nThis example illustrates a simple Prolog program that defines facts, rules, and queries. The program is designed to answer the question of whether Socrates is mortal based on the fact that he is human. In practice, Prolog programs can be much more complex, with many facts, rules, and queries that define a domain of interest.\nIn the code below, everything in a line after a ‘%’ is a comment that explains what the code is doing. Comments are not part of the program itself but are there to help you understand the code.\n% Facts\nhuman(socrates).\n\n% Rules\nmortal(X) :- human(X). \n\n% Queries\n?- mortal(socrates).\n\nExplanation:\n\nFacts:\nIn Prolog, facts are basic statements about things we know to be true. In this case, the fact human(socrates). means that Socrates is a human. This is a piece of information we’re giving to the program. In a medical context, we might have facts like symptom(fever). or history_of(cancer). that represent information about patients.\nRules:\nRules in Prolog define relationships between facts. Here, we have the rule mortal(X) :- human(X)., which means “X is mortal if X is human.” “Variables” in prolog begin with capital letters, so X is a variable. In Prolog, the :- symbol can be read as “if.” So, this rule is saying, “if X is human, then X is mortal.” This is a simple logical relationship that Prolog can use to make inferences.\nQueries:\nWhen we want to ask Prolog a question (or query), we do so by providing a query. For example, the query ?- mortal(socrates). is asking, “Is Socrates mortal?” Prolog will use the facts and rules we’ve provided to answer this question.\nIn this case, it looks at the rule mortal(X) :- human(X). and sees that since Socrates is a human (from the fact human(socrates).), he must also be mortal. So, Prolog will respond with “Yes” or true.\n\n\n\nHow It Works:\n\nhuman(socrates).: This is a fact we already know—Socrates is human.\nmortal(X) :- human(X).: This is a rule that says any human is mortal.\n?- mortal(socrates).: This is the query we ask, and Prolog checks the facts and rules to conclude that Socrates is mortal because he is human.\n\nIn summary, this small Prolog program uses logic to infer that Socrates is mortal based on the information that he’s human. It reflects how Prolog works by defining facts, applying rules, and answering queries logically."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#knowledge-base",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#knowledge-base",
    "title": "Play with Prolog",
    "section": "Knowledge Base",
    "text": "Knowledge Base\nThe first step in building an expert system is to define the knowledge base. This knowledge base will contain the facts and rules that the system will use to reason and make decisions. In our case, we are going to build an expert system that can diagnose common conditions like colds, flu, and allergies based on a set of symptoms.\nWe are going to limit our facts to the following symptoms:\n\nrunny nose\nsneezing\nfever\nfatigue\n\nThese are common symptoms that can help differentiate between colds, flu, and allergies. For this exercise, lets assume that the following rules apply:\n\nIf a patient has a runny nose and sneezing with or without a fever, they likely have a common cold.\nIf a patient has a fever, sore throat, and runny nose, they likely have the flu.\nIf a patient has sneezing and a runny nose but no fever, they likely have allergies.\n\nWe can define these symptoms as facts in Prolog, like this:\n\n\n\nSymptom\nProlog Fact Representation\n\n\n\n\nRunny Nose\nsymptom(runny_nose).\n\n\nSneezing\nsymptom(sneezing).\n\n\nFever\nsymptom(fever).\n\n\nFatigue\nsymptom(fatigue).\n\n\nItchy Eyes\nsymptom(itchy_eyes).\n\n\n\nWe are going to simplify the rules for the sake of this exercise, but in a real-world scenario, the rules would be more complex and based on a larger set of symptoms and diagnostic criteria. In this case, we’ll assume that the presence or absence of these symptoms is enough to differentiate between the conditions.\nFor the common cold, we’ll use the following rule:\n\nIf a patient has a runny nose and sneezing but no fever, they likely have a common cold.\nTreatment: Rest, drink fluids, consider over-the-counter cold medicine.\n\nFor the flu, we’ll use the following rule:\n\nIf a patient has a fever, fatigue, and runny nose, they likely have the flu.\nTreatment: Rest, stay hydrated, take anti-fever medication, see a doctor if symptoms worsen.\n\nFor allergies, we’ll use the following rule:\n\nIf a patient has sneezing, a runny nose, and itchy eyes, they likely have allergies.\nTreatment: Avoid allergens, use antihistamines, consult an allergist if needed."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#visualizing-the-expert-system",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#visualizing-the-expert-system",
    "title": "Play with Prolog",
    "section": "Visualizing the Expert System",
    "text": "Visualizing the Expert System\nBefore we dive into the Prolog code, let’s visualize the expert system using a flowchart. This will help us understand the logic and decision-making process that the system will follow. The flowchart will show how the system will diagnose patients based on their symptoms and recommend appropriate treatments.\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B{Runny nose?}\n    B --&gt;|No| Z[No diagnosis]\n\n    subgraph Symptom_Check\n        B --&gt;|Yes| C{Sneezing?}\n        C --&gt;|No| Z\n        C --&gt;|Yes| D{Fever?}\n        D --&gt;|Yes| E[Flu]\n        D --&gt;|No| F{Itchy eyes?}\n        F --&gt;|Yes| G[Allergies]\n        F --&gt;|No| H[Common Cold]\n    end\n\n    subgraph Diagnosis\n        E --&gt; I[Diagnosis: Flu]\n        G --&gt; J[Diagnosis: Allergies]\n        H --&gt; K[Diagnosis: Common Cold]\n    end\n\n    subgraph Treatment\n        I --&gt; L[Treatment: Rest, stay hydrated, take anti-fever medication, see doctor if symptoms worsen]\n        J --&gt; M[Treatment: Avoid allergens, use antihistamines, consult allergist if needed]\n        K --&gt; N[Treatment: Rest, drink fluids, consider over-the-counter cold medicine]\n    end"
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#reasoning-engine",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#reasoning-engine",
    "title": "Play with Prolog",
    "section": "Reasoning engine",
    "text": "Reasoning engine\nOnce we have defined our knowledge base, we need some kind of “reaoning” or “inference” engine to process the information and draw conclusions. This is where the Prolog programming language comes in. Prolog is a logic programming language that is well-suited for representing and reasoning with complex knowledge and rules. It uses a declarative syntax that allows you to define relationships, rules, and facts in a concise and intuitive way.\nProlog programs consist of a set of facts and rules that define the relationships between different entities in the system. The Prolog interpreter uses these facts and rules to answer queries and make inferences based on the input data.\nHere are some facts that we might include in our Prolog program based on some symptoms that a patient might present with:\n% Facts\nsymptom(fever).\nsymptom(fatigue).\nsymptom(runny_nose).\nsymptom(sneezing).\nsymptom(itchy_eyes).\nWhen we want to use Prolog, we tell the system what we know (facts) and what we want to know (queries). The system then uses the rules and facts to answer our queries.\nAnd here are some rules that we might include in our Prolog program based on the diagnostic criteria for different conditions:\n% Rules for diagnossis\n% Diagnosis rules\ndiagnose(common_cold) :-\n    symptom(runny_nose),\n    symptom(sneezing),\n    \\+ symptom(fever),\n    write('Diagnosis: Common Cold'), nl.\n\ndiagnose(flu) :-\n    symptom(fever),\n    symptom(fatigue),\n    symptom(runny_nose),\n    write('Diagnosis: Flu'), nl.\n\ndiagnose(allergies) :-\n    symptom(sneezing),\n    symptom(runny_nose),\n    symptom(itchy_eyes),\n    write('Diagnosis: Allergies'), nl.\nAnd we can do the same for the treatment recommendations:\n% Rules for treatment\ntreatment(common_cold) :-\n    write('Treatment: Rest, drink plenty of fluids, and consider over-the-counter cold medicine.'), nl.\n\ntreatment(flu) :-\n    write('Treatment: Rest, stay hydrated, take anti-fever medication, and see a doctor if symptoms worsen.'), nl.\n\ntreatment(allergies) :-\n    write('Treatment: Avoid allergens, use antihistamines, and consult an allergist if needed.'), nl.\nFinally, we can put it all together in a Prolog program that checks the symptoms, makes a diagnosis, and provides treatment recommendations:\n% Rule to check diagnosis and apply treatment\ncheck_diagnosis_and_treatment :-\n    diagnose(Disease),\n    treatment(Disease)."
  },
  {
    "objectID": "Exercises/aom-expert-system/play-with-prolog.html#footnotes",
    "href": "Exercises/aom-expert-system/play-with-prolog.html#footnotes",
    "title": "Play with Prolog",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://www.datacamp.com/blog/what-is-narrow-ai for more information on narrow AI.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IDPT-8079: Frontier of AI in Medicine",
    "section": "",
    "text": "This is a collection of talks on various topics in machine learning and artificial intelligence. It also includes some exercises and interactive demos."
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "IDPT-8079: Frontier of AI in Medicine",
    "section": "Exercises",
    "text": "Exercises\n\nBuild an expert system using Prolog"
  },
  {
    "objectID": "index.html#talks",
    "href": "index.html#talks",
    "title": "IDPT-8079: Frontier of AI in Medicine",
    "section": "Talks",
    "text": "Talks\n\nAI History\nIntroduction to Artificial Intelligence and Machine Learning"
  },
  {
    "objectID": "talks/ai-history/index.html#introduction",
    "href": "talks/ai-history/index.html#introduction",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Introduction",
    "text": "Introduction\n\nArtificial Intelligence (AI) and Machine Learning (ML) have a rich history\nFrom early concepts to modern applications\nThis presentation covers key milestones and breakthroughs"
  },
  {
    "objectID": "talks/ai-history/index.html#early-beginnings-1940s-1950s",
    "href": "talks/ai-history/index.html#early-beginnings-1940s-1950s",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Early Beginnings (1940s-1950s)",
    "text": "Early Beginnings (1940s-1950s)\n\n\n\n1936: Turing and the Computable Numbers (Turing 1936)\n1943: McCulloch and Pitts create a computational model for neural networks\n1950: Alan Turing proposes the Turing Test (Turing 1950) and Paper\n1956: Dartmouth Conference coins the term “Artificial Intelligence”"
  },
  {
    "objectID": "talks/ai-history/index.html#i-robot-by-isaac-asimov",
    "href": "talks/ai-history/index.html#i-robot-by-isaac-asimov",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "“I, Robot” by Isaac Asimov",
    "text": "“I, Robot” by Isaac Asimov\n\n\n\nPublished in 1950 by Gnome Press\nCollection of nine science fiction short stories\nOriginally appeared in super-science fiction magazines (1940-1950)\nIntroduced the concept of positronic robots and the Three Laws of Robotics\n\n\n\n\nAsimov (1950)\n\nAsimov’s work laid the foundation for ethical considerations in AI development."
  },
  {
    "objectID": "talks/ai-history/index.html#the-three-laws-of-robotics",
    "href": "talks/ai-history/index.html#the-three-laws-of-robotics",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "The Three Laws of Robotics",
    "text": "The Three Laws of Robotics\n\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n\n\nAsimov later added the Zeroth Law that superseded the others:\n\nA robot may not harm humanity, or, by inaction, allow humanity to come to harm.\n\n\n\nThese laws have become a cornerstone in discussions about AI ethics and safety."
  },
  {
    "objectID": "talks/ai-history/index.html#i-robots-influence-on-modern-ai",
    "href": "talks/ai-history/index.html#i-robots-influence-on-modern-ai",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "“I, Robot”’s Influence on Modern AI",
    "text": "“I, Robot”’s Influence on Modern AI\n\nSparked discussions on machine ethics and AI safety\nInfluenced researchers to consider ethical implications of AI development\nConcept of “friendly AI” draws parallels to Asimov’s laws\nChallenges presented in stories mirror real-world AI alignment problems\n\n\nWhile not directly implementable, Asimov’s laws have shaped thinking about AI governance and ethics in the field."
  },
  {
    "objectID": "talks/ai-history/index.html#the-turing-test",
    "href": "talks/ai-history/index.html#the-turing-test",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "1950: The Turing Test",
    "text": "1950: The Turing Test\n\nWho’s the real human?"
  },
  {
    "objectID": "talks/ai-history/index.html#the-golden-years-1956-1974",
    "href": "talks/ai-history/index.html#the-golden-years-1956-1974",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "The Golden Years (1956-1974)",
    "text": "The Golden Years (1956-1974)\n\nDevelopment of early AI programs\n1957: Frank Rosenblatt develops the Perceptron\n1964: ELIZA, one of the first chatbots, is created by Joseph Weizenbaum"
  },
  {
    "objectID": "talks/ai-history/index.html#eliza-one-of-the-first-chatbots",
    "href": "talks/ai-history/index.html#eliza-one-of-the-first-chatbots",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "1964: ELIZA, one of the first chatbots",
    "text": "1964: ELIZA, one of the first chatbots\n\n\n\n\n\n\n\n\nAlthough ELIZA was limited in terms of actual understanding, it marked an important milestone in the development of AI and human-computer interaction, showing how conversation-based interfaces could influence the perception of intelligence.\n\n\nELIZA was one of the earliest chatbots, developed in the mid-1960s by Joseph Weizenbaum, a computer scientist at MIT. The program was designed to simulate a conversation between a human and a machine, and it did so by using pattern matching and substitution methodology, a simple but effective form of natural language processing.\nELIZA’s most famous script was called “DOCTOR,” which mimicked a Rogerian psychotherapist. In this role, ELIZA would take users’ input and reflect their statements back to them in a way that encouraged further conversation. For example, if a user said, “I’m feeling sad today,” ELIZA might respond with, “Why do you think you’re feeling sad today?”\nWhile ELIZA’s responses were largely superficial, many users were surprised at how human-like they seemed. Weizenbaum created ELIZA to demonstrate how easily people could attribute human-like understanding to a machine, even when its responses were formulaic. He was struck by how quickly people became emotionally attached to the program, despite knowing it was not genuinely intelligent.\nAlthough ELIZA was limited in terms of actual understanding, it marked an important milestone in the development of AI and human-computer interaction, showing how conversation-based interfaces could influence the perception of intelligence."
  },
  {
    "objectID": "talks/ai-history/index.html#what-is-an-expert-system",
    "href": "talks/ai-history/index.html#what-is-an-expert-system",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "What is an Expert System?",
    "text": "What is an Expert System?\n\nDefinition: Computer program that emulates decision-making ability of a human expert\nKey Components:\n\nKnowledge Base: Contains domain-specific information and rules\nInference Engine: Applies rules to the knowledge to derive new information\nUser Interface: Allows non-expert users to interact with the system\n\nCharacteristics:\n\nSolves complex problems by reasoning through bodies of knowledge\nSeparates domain knowledge from the reasoning mechanism\nCan explain its decisions and reasoning"
  },
  {
    "objectID": "talks/ai-history/index.html#what-is-an-expert-system-1",
    "href": "talks/ai-history/index.html#what-is-an-expert-system-1",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "What is an Expert System?",
    "text": "What is an Expert System?\n\nApplications:\n\nMedical diagnosis (e.g., MYCIN)\nFinancial planning\nManufacturing process control\nScientific analysis\n\nAdvantages:\n\nConsistent and accurate decisions\nPreservation of expert knowledge\nAbility to handle complex scenarios\n\nLimitations:\n\nLimited to specific domains (narrow AI)\nDifficulty in capturing tacit knowledge\nMay struggle with unusual or unprecedented situations"
  },
  {
    "objectID": "talks/ai-history/index.html#example-expert-system-mycin",
    "href": "talks/ai-history/index.html#example-expert-system-mycin",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Example Expert system: MYCIN",
    "text": "Example Expert system: MYCIN\n\nDeveloped in the early 1970s at Stanford University\nOne of the first rule-based expert systems in medicine\nPurpose: Assist physicians in diagnosing and treating bacterial infections\nFocused on bloodstream infections (bacteremia and meningitis)\nNamed after antibiotics (many of which end in “-mycin”)"
  },
  {
    "objectID": "talks/ai-history/index.html#mycin-key-features-and-functionality",
    "href": "talks/ai-history/index.html#mycin-key-features-and-functionality",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "MYCIN: Key Features and Functionality",
    "text": "MYCIN: Key Features and Functionality\n\nRule-based system with approximately 600 rules\nUsed backward chaining inference engine\nIncorporated certainty factors to handle uncertainty\nAsked users a series of yes/no questions about symptoms and test results\nProvided diagnosis recommendations and suggested antibiotic treatments\nExplained its reasoning process to user"
  },
  {
    "objectID": "talks/ai-history/index.html#mycin-impact-and-legacy",
    "href": "talks/ai-history/index.html#mycin-impact-and-legacy",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "MYCIN: Impact and Legacy",
    "text": "MYCIN: Impact and Legacy\n\nNever used in clinical practice due to ethical and legal concerns\nAchieved performance comparable to human experts in its domain\nPioneered several important concepts in AI and expert systems:\n\nSeparation of knowledge base from inference engine\nExplanation of reasoning\nHandling of uncertainty\n\nInfluenced development of subsequent expert systems and clinical decision support tools\nDemonstrated potential of AI in healthcare, paving way for modern medical AI applications"
  },
  {
    "objectID": "talks/ai-history/index.html#increased-computational-power",
    "href": "talks/ai-history/index.html#increased-computational-power",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Increased Computational Power",
    "text": "Increased Computational Power\n\nAdvancement:\n\nRapid growth of CPUs and the emergence of GPUs (Graphics Processing Units).\n\nImpact:\n\nEnabled the training of deeper neural networks essential for various AI tasks."
  },
  {
    "objectID": "talks/ai-history/index.html#big-data",
    "href": "talks/ai-history/index.html#big-data",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Big Data",
    "text": "Big Data\n\nAdvancement:\n\nExplosion of digital data from the internet, social media, and sensors.\n\nImpact:\n\nFacilitated the development of accurate models as ML algorithms require substantial data to learn effectively."
  },
  {
    "objectID": "talks/ai-history/index.html#open-source-frameworks-and-libraries",
    "href": "talks/ai-history/index.html#open-source-frameworks-and-libraries",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Open Source Frameworks and Libraries",
    "text": "Open Source Frameworks and Libraries\n\nAdvancement:\n\nEmergence of libraries like TensorFlow (2015), Keras (2015), and Scikit-learn (2007).\n\nImpact:\n\nLowered the barrier for AI development, allowing more practitioners to innovate in the field."
  },
  {
    "objectID": "talks/ai-history/index.html#advances-in-algorithms",
    "href": "talks/ai-history/index.html#advances-in-algorithms",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Advances in Algorithms",
    "text": "Advances in Algorithms\n\nAdvancement:\n\nResearch in new algorithms, such as support vector machines and deep learning architectures.\n\nImpact:\n\nImproved AI performance across applications, particularly in image and speech recognition."
  },
  {
    "objectID": "talks/ai-history/index.html#cloud-computing",
    "href": "talks/ai-history/index.html#cloud-computing",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Cloud Computing",
    "text": "Cloud Computing\n\nAdvancement:\n\nRise of cloud computing platforms (e.g., AWS, Google Cloud, Microsoft Azure).\n\nImpact:\n\nProvided scalable resources for storage and computation, enabling extensive ML experimentation."
  },
  {
    "objectID": "talks/ai-history/index.html#collaborative-research-and-knowledge-sharing",
    "href": "talks/ai-history/index.html#collaborative-research-and-knowledge-sharing",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Collaborative Research and Knowledge Sharing",
    "text": "Collaborative Research and Knowledge Sharing\n\nAdvancement:\n\nIncreased collaboration and sharing of findings through conferences and online platforms.\n\nImpact:\n\nAccelerated innovation in AI and ML as researchers built upon each other’s work."
  },
  {
    "objectID": "talks/ai-history/index.html#investment-and-interest-from-industry",
    "href": "talks/ai-history/index.html#investment-and-interest-from-industry",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Investment and Interest from Industry",
    "text": "Investment and Interest from Industry\n\nAdvancement:\n\nGrowing interest and investment from tech giants and startups in AI technologies.\n\nImpact:\n\nLed to the development of practical applications and commercial products, driving further research."
  },
  {
    "objectID": "talks/ai-history/index.html#deep-learning-resurgence-2006",
    "href": "talks/ai-history/index.html#deep-learning-resurgence-2006",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Deep Learning Resurgence (2006)",
    "text": "Deep Learning Resurgence (2006)\n\nMilestone: Geoffrey Hinton and team introduced “deep belief networks.”\nImpact: Marked the resurgence of deep learning and laid the foundation for modern AI applications, especially in image and speech recognition."
  },
  {
    "objectID": "talks/ai-history/index.html#alexnet-wins-imagenet-competition-2012",
    "href": "talks/ai-history/index.html#alexnet-wins-imagenet-competition-2012",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "AlexNet Wins ImageNet Competition (2012)",
    "text": "AlexNet Wins ImageNet Competition (2012)\n\nMilestone: Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton’s deep neural network (AlexNet) won the ImageNet competition.\nImpact: Showcased the power of convolutional neural networks (CNNs) and triggered widespread adoption in computer vision tasks."
  },
  {
    "objectID": "talks/ai-history/index.html#ibm-watson-wins-jeopardy-2011",
    "href": "talks/ai-history/index.html#ibm-watson-wins-jeopardy-2011",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "IBM Watson Wins Jeopardy! (2011)",
    "text": "IBM Watson Wins Jeopardy! (2011)\n\nMilestone: IBM Watson defeated champions Ken Jennings and Brad Rutter on Jeopardy!.\nImpact: Demonstrated AI’s ability to process and understand natural language, leading to applications in healthcare, finance, and customer service."
  },
  {
    "objectID": "talks/ai-history/index.html#md-anderson-sets-watson-aside-2017-2018",
    "href": "talks/ai-history/index.html#md-anderson-sets-watson-aside-2017-2018",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "MD Anderson sets Watson aside (2017-2018)",
    "text": "MD Anderson sets Watson aside (2017-2018)\n\nSee Herper (n.d.)"
  },
  {
    "objectID": "talks/ai-history/index.html#generative-adversarial-networks-gans-2014",
    "href": "talks/ai-history/index.html#generative-adversarial-networks-gans-2014",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Generative Adversarial Networks (GANs) (2014)",
    "text": "Generative Adversarial Networks (GANs) (2014)\n\nMilestone: Ian Goodfellow introduced GANs, a model where two neural networks compete to generate realistic data.\nImpact: Revolutionized image generation and unsupervised learning, powering innovations like deepfakes and AI-generated art."
  },
  {
    "objectID": "talks/ai-history/index.html#alphago-defeats-world-champion-2016",
    "href": "talks/ai-history/index.html#alphago-defeats-world-champion-2016",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "AlphaGo Defeats World Champion (2016)",
    "text": "AlphaGo Defeats World Champion (2016)\n\nMilestone: Google DeepMind’s AlphaGo defeated Go champion Lee Sedol.\nImpact: Showcased the capability of reinforcement learning and deep neural networks in mastering complex strategic games."
  },
  {
    "objectID": "talks/ai-history/index.html#transformer-architecture-2017",
    "href": "talks/ai-history/index.html#transformer-architecture-2017",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Transformer Architecture (2017)",
    "text": "Transformer Architecture (2017)\n\nMilestone: Vaswani et al. introduced the Transformer model, revolutionizing natural language processing.\nImpact: Laid the groundwork for state-of-the-art NLP models like BERT and GPT, transforming language understanding and generation."
  },
  {
    "objectID": "talks/ai-history/index.html#ai-and-ml-in-social-media-2010s",
    "href": "talks/ai-history/index.html#ai-and-ml-in-social-media-2010s",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "AI and ML in Social Media (2010s)",
    "text": "AI and ML in Social Media (2010s)\n\nMilestone: Social media platforms adopted AI for content recommendation and moderation.\nImpact: Enhanced user engagement and experience, but also raised concerns about echo chambers, misinformation, and algorithmic bias."
  },
  {
    "objectID": "talks/ai-history/index.html#alphafold-solves-protein-folding-2020",
    "href": "talks/ai-history/index.html#alphafold-solves-protein-folding-2020",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "AlphaFold Solves Protein Folding (2020)",
    "text": "AlphaFold Solves Protein Folding (2020)\n\nMilestone: DeepMind’s AlphaFold achieved breakthrough accuracy in predicting protein structures.\nImpact: Solved a 50-year-old challenge in biology, opening new doors in drug discovery and molecular biology."
  },
  {
    "objectID": "talks/ai-history/index.html#gpt-4-and-large-language-models-2023",
    "href": "talks/ai-history/index.html#gpt-4-and-large-language-models-2023",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "GPT-4 and Large Language Models (2023)",
    "text": "GPT-4 and Large Language Models (2023)\n\nMilestone: OpenAI’s GPT-4 showcased the potential of large-scale language models for complex, nuanced language understanding.\nImpact: Accelerated the development of AI-driven content creation and enhanced human-computer interaction."
  },
  {
    "objectID": "talks/ai-history/index.html#future-directions",
    "href": "talks/ai-history/index.html#future-directions",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Future Directions",
    "text": "Future Directions\n\nArtificial General Intelligence (AGI) research\nQuantum computing and AI\nNeuromorphic computing\nHuman-AI collaboration\n\nFor deeper dive into the history, see (Norman 2024)."
  },
  {
    "objectID": "talks/ai-history/index.html#challenges-and-opportunities",
    "href": "talks/ai-history/index.html#challenges-and-opportunities",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "Challenges and Opportunities",
    "text": "Challenges and Opportunities\n\nEthical AI development\nAI governance and regulation\nAddressing AI bias and fairness\nBalancing innovation with responsible development"
  },
  {
    "objectID": "talks/ai-history/index.html#references",
    "href": "talks/ai-history/index.html#references",
    "title": "The History of Artificial Intelligence and Machine Learning",
    "section": "References",
    "text": "References\n\n\n\n\nAsimov, Isaac. 1950. I, Robot. Bantam hardcover ed. (2). New York: Bantam Books.\n\n\nHerper, Matthew. n.d. “MD Anderson Benches IBM Watson In Setback For Artificial Intelligence In Medicine.” Forbes. Accessed October 2, 2024. https://www.forbes.com/sites/matthewherper/2017/02/19/md-anderson-benches-ibm-watson-in-setback-for-artificial-intelligence-in-medicine/.\n\n\nNorman, Jeremy. 2024. “History of Information.” https://www.historyofinformation.com/?cat=71.\n\n\nTuring, Alan. 1936. “On Computable Numbers, with an Application to the Entscheidungsproblem.” https://www.abelard.org/turpap2/tp2-ie.asp.\n\n\n———. 1950. “Computing Machinery and Intelligence.” Mind 59 (October): 433–60. https://doi.org/10.1093/mind/lix.236.433."
  },
  {
    "objectID": "talks/ml-intro/index.html#outline",
    "href": "talks/ml-intro/index.html#outline",
    "title": "Introduction to Machine Learning",
    "section": "Outline",
    "text": "Outline\n\nRelationship between AI and ML\nMachine Learning Uses\nTypes of Machine Learning\nTraining and Testing\nGeneralizability\nBias and Variance\nKey Concepts in ML\nChallenges and Future Directions"
  },
  {
    "objectID": "talks/ml-intro/index.html#an-ontology-of-ai",
    "href": "talks/ml-intro/index.html#an-ontology-of-ai",
    "title": "Introduction to Machine Learning",
    "section": "An Ontology of AI",
    "text": "An Ontology of AI"
  },
  {
    "objectID": "talks/ml-intro/index.html#what-is-machine-learning",
    "href": "talks/ml-intro/index.html#what-is-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\nThe study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning systems give the computer the ability to learn without being explicitly programmed rules."
  },
  {
    "objectID": "talks/ml-intro/index.html#what-is-deep-learning",
    "href": "talks/ml-intro/index.html#what-is-deep-learning",
    "title": "Introduction to Machine Learning",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\nMachine learning algorithms that are inspired by the structure and function of the brain. Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is often unstructured (i.e., text or images)."
  },
  {
    "objectID": "talks/ml-intro/index.html#ai-vs.-ml-vs.-deep-learning",
    "href": "talks/ml-intro/index.html#ai-vs.-ml-vs.-deep-learning",
    "title": "Introduction to Machine Learning",
    "section": "AI vs. ML vs. Deep Learning",
    "text": "AI vs. ML vs. Deep Learning\n\n\nArtificialIntelligence\n\n\nMachineLearning\n\n\nDeepLearning"
  },
  {
    "objectID": "talks/ml-intro/index.html#ai-vs.-ml-vs.-deep-learning-1",
    "href": "talks/ml-intro/index.html#ai-vs.-ml-vs.-deep-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "AI vs. ML vs. Deep Learning",
    "text": "AI vs. ML vs. Deep Learning\n\n\nArtificialIntelligence\n\n\nMachineLearning\n\n\nDeepLearning"
  },
  {
    "objectID": "talks/ml-intro/index.html#an-ontology-of-data",
    "href": "talks/ml-intro/index.html#an-ontology-of-data",
    "title": "Introduction to Machine Learning",
    "section": "An ontology of Data",
    "text": "An ontology of Data\n\n\n\n\nOne way to think about different classes of data."
  },
  {
    "objectID": "talks/ml-intro/index.html#structured-data-vs-unstructured-data",
    "href": "talks/ml-intro/index.html#structured-data-vs-unstructured-data",
    "title": "Introduction to Machine Learning",
    "section": "Structured Data vs Unstructured Data",
    "text": "Structured Data vs Unstructured Data\n\n\n\n\n\n\n\n\n\nStructured data\nUnstructured data\n\n\n\n\nWhat is it?\nData that fits in a predefined data model or schema.\nData without an underlying model to discern attributes.\n\n\nBasic example\nAn Excel table.\nA collection of video files.\n\n\nBest for\nA set of predefined observations or characteristics (columns) on a collection of things (rows)\nAn associated collection of data, objects, or files where the attributes change or are unknown.\n\n\nCommon formats\nCSV, TSV, Excel, SQL databases.\nImages, audio, video, text."
  },
  {
    "objectID": "talks/ml-intro/index.html#tabular-data",
    "href": "talks/ml-intro/index.html#tabular-data",
    "title": "Introduction to Machine Learning",
    "section": "Tabular Data",
    "text": "Tabular Data\n\nRows and columns\nEach column has a specific data type\nEach row represents an observation on a subject (e.g., patient, sample)\n\nTerminology: - Feature: A column in the dataset - Target: The variable you are trying to predict - Observation: A row in the dataset - Dimension: The number of features in the dataset (sometimes represented by \\(p\\)) - Sample: A single row in the dataset (usually represented by \\(n\\))"
  },
  {
    "objectID": "talks/ml-intro/index.html#classes-of-machine-learning",
    "href": "talks/ml-intro/index.html#classes-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Classes of Machine Learning",
    "text": "Classes of Machine Learning\n\nBroad classes of machine learning."
  },
  {
    "objectID": "talks/ml-intro/index.html#types-of-machine-learning",
    "href": "talks/ml-intro/index.html#types-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nSupervised Learning\n\nClassification\nRegression\n\nUnsupervised Learning\n\nClustering\nDimensionality Reduction\n\nReinforcement Learning"
  },
  {
    "objectID": "talks/ml-intro/index.html#reinforecement-learning",
    "href": "talks/ml-intro/index.html#reinforecement-learning",
    "title": "Introduction to Machine Learning",
    "section": "Reinforecement Learning",
    "text": "Reinforecement Learning\n\nLearning through interaction: Unlike supervised learning where you have labeled data, RL agents learn by interacting with an environment.\nDelayed rewards: The agent doesn’t receive immediate feedback about the optimal action, but rather must learn which actions lead to better cumulative rewards over time.\nExploration vs. exploitation: The agent must balance exploring new actions versus exploiting known good actions.\nSequential decision making: RL deals with sequences of decisions rather than one-time predictions.\n\n\nExamples\n\n\n\nGame playing (e.g., AlphaGo, OpenAI Five)\nRobotics control\n\n\n\nAutonomous driving\nTrading strategies"
  },
  {
    "objectID": "talks/ml-intro/index.html#a-map-of-machine-learning-approaches",
    "href": "talks/ml-intro/index.html#a-map-of-machine-learning-approaches",
    "title": "Introduction to Machine Learning",
    "section": "A map of machine learning approaches",
    "text": "A map of machine learning approaches\n\n\nhttps://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
  },
  {
    "objectID": "talks/ml-intro/index.html#supervised-learning-1",
    "href": "talks/ml-intro/index.html#supervised-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nGiven a dataset of input-output pairs: \\(\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}\\)\nLearn a function \\(f\\) with parameters \\(\\theta\\) that maps inputs \\(x\\) to outputs \\(y\\) \\[y = f(x; \\theta) + \\epsilon\\]\nObjective: Choose parameters \\(\\theta\\) to minimize the prediction error \\(\\epsilon\\)\n\nWhen \\(y\\) is a continuous variable (e.g., house prices), it’s a regression problem.\nWhen \\(y\\) is a discrete variable (e.g., spam or not spam), it’s a classification problem."
  },
  {
    "objectID": "talks/ml-intro/index.html#classification",
    "href": "talks/ml-intro/index.html#classification",
    "title": "Introduction to Machine Learning",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "talks/ml-intro/index.html#regression",
    "href": "talks/ml-intro/index.html#regression",
    "title": "Introduction to Machine Learning",
    "section": "Regression",
    "text": "Regression\n\nNot all regression is linear…."
  },
  {
    "objectID": "talks/ml-intro/index.html#classification-example",
    "href": "talks/ml-intro/index.html#classification-example",
    "title": "Introduction to Machine Learning",
    "section": "Classification Example",
    "text": "Classification Example\n\nThe classic iris"
  },
  {
    "objectID": "talks/ml-intro/index.html#classification-example-1",
    "href": "talks/ml-intro/index.html#classification-example-1",
    "title": "Introduction to Machine Learning",
    "section": "Classification Example",
    "text": "Classification Example\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor"
  },
  {
    "objectID": "talks/ml-intro/index.html#classification-example-2",
    "href": "talks/ml-intro/index.html#classification-example-2",
    "title": "Introduction to Machine Learning",
    "section": "Classification Example",
    "text": "Classification Example"
  },
  {
    "objectID": "talks/ml-intro/index.html#training-and-testing",
    "href": "talks/ml-intro/index.html#training-and-testing",
    "title": "Introduction to Machine Learning",
    "section": "Training and Testing",
    "text": "Training and Testing\n\n\nTraining Data: Used to teach the model. It consists of input-output pairs for a supervised learning task.\nTesting Data: Used to evaluate the model’s performance. It should be separate from the training data, but come from the same distribution or population.\nValidation Data: Used for tuning hyperparameters. Hyperparameters are settings that control the learning process (e.g., k in k-NN, learning rate in neural networks)."
  },
  {
    "objectID": "talks/ml-intro/index.html#generalizability",
    "href": "talks/ml-intro/index.html#generalizability",
    "title": "Introduction to Machine Learning",
    "section": "Generalizability",
    "text": "Generalizability\n\nThe ability of a model to perform well on unseen data\n\nThis table shows the relationship between training error, testing error, and overfitting, good fit, and underfitting.\n\n\n\nTraining Error\nTesting Error\nModel Fit\n\n\n\n\nLow\nHigh\nOverfit\n\n\nLow\nLow\nGood Fit\n\n\nHigh\nHigh\nUnderfit (poor performance)"
  },
  {
    "objectID": "talks/ml-intro/index.html#bias-and-variance",
    "href": "talks/ml-intro/index.html#bias-and-variance",
    "title": "Introduction to Machine Learning",
    "section": "Bias and Variance",
    "text": "Bias and Variance\n\nBias: Error from incorrect assumptions in the learning algorithm\nVariance: Error from sensitivity to small fluctuations in the training set\nBias-Variance Tradeoff: The conflict in trying to simultaneously minimize these two sources of error"
  },
  {
    "objectID": "talks/ml-intro/index.html#algorithms-for-supervised-learning",
    "href": "talks/ml-intro/index.html#algorithms-for-supervised-learning",
    "title": "Introduction to Machine Learning",
    "section": "Algorithms for Supervised Learning",
    "text": "Algorithms for Supervised Learning\n\nLinear regression."
  },
  {
    "objectID": "talks/ml-intro/index.html#algorithms-for-supervised-learning-1",
    "href": "talks/ml-intro/index.html#algorithms-for-supervised-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "Algorithms for Supervised Learning",
    "text": "Algorithms for Supervised Learning\n\nAnscombe’s Quartet\nhttps://en.wikipedia.org/wiki/Anscombe%27s_quartet"
  },
  {
    "objectID": "talks/ml-intro/index.html#algorithms-for-supervised-learning-2",
    "href": "talks/ml-intro/index.html#algorithms-for-supervised-learning-2",
    "title": "Introduction to Machine Learning",
    "section": "Algorithms for Supervised Learning",
    "text": "Algorithms for Supervised Learning\n\nClassification and Regression Trees (CART)."
  },
  {
    "objectID": "talks/ml-intro/index.html#algorithms-for-supervised-learning-3",
    "href": "talks/ml-intro/index.html#algorithms-for-supervised-learning-3",
    "title": "Introduction to Machine Learning",
    "section": "Algorithms for Supervised Learning",
    "text": "Algorithms for Supervised Learning\n\nK-nearest neighbor (kNN) algorithm."
  },
  {
    "objectID": "talks/ml-intro/index.html#algorithms-for-supervised-learning-4",
    "href": "talks/ml-intro/index.html#algorithms-for-supervised-learning-4",
    "title": "Introduction to Machine Learning",
    "section": "Algorithms for Supervised Learning",
    "text": "Algorithms for Supervised Learning\n\nRandom Forests."
  },
  {
    "objectID": "talks/ml-intro/index.html#algorithms-for-supervised-learning-5",
    "href": "talks/ml-intro/index.html#algorithms-for-supervised-learning-5",
    "title": "Introduction to Machine Learning",
    "section": "Algorithms for Supervised Learning",
    "text": "Algorithms for Supervised Learning\n\nDeep learning."
  },
  {
    "objectID": "talks/ml-intro/index.html#applying-supervised-learning-algorithms",
    "href": "talks/ml-intro/index.html#applying-supervised-learning-algorithms",
    "title": "Introduction to Machine Learning",
    "section": "Applying Supervised Learning Algorithms",
    "text": "Applying Supervised Learning Algorithms\n\nThe mlr3 ecosystem in R."
  },
  {
    "objectID": "talks/ml-intro/index.html#key-concepts-in-supervised-learning",
    "href": "talks/ml-intro/index.html#key-concepts-in-supervised-learning",
    "title": "Introduction to Machine Learning",
    "section": "Key Concepts in Supervised Learning",
    "text": "Key Concepts in Supervised Learning\n\nFeature Engineering\nModel Selection\nHyperparameter Tuning\nEnsemble Methods\nCross-Validation\nEvaluation Metrics\n\n\nBriefly explain each concept: 1. Feature Engineering: Creating new features or transforming existing ones to improve model performance 2. Model Selection: Choosing the appropriate algorithm for your problem 3. Hyperparameter Tuning: Optimizing model parameters that are not learned from data 4. Ensemble Methods: Combining multiple models to improve performance 5. Cross-Validation: Technique for assessing how the model will generalize to an independent dataset 6. Evaluation Metrics: Different ways to measure model performance (accuracy, precision, recall, F1-score, ROC curve, etc.)"
  },
  {
    "objectID": "talks/ml-intro/index.html#challenges-in-machine-learning",
    "href": "talks/ml-intro/index.html#challenges-in-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Challenges in Machine Learning",
    "text": "Challenges in Machine Learning\n\nData Quality and Quantity\nInterpretability vs Performance\nEthical Considerations and Bias\nComputational Resources\nModel Deployment and Maintenance\n\n\nDiscuss each challenge: 1. The importance of good, representative data and the challenges of data collection and cleaning 2. The tradeoff between complex, high-performing models and simpler, more interpretable ones 3. The risk of perpetuating or amplifying societal biases through ML models 4. The need for significant computational power, especially for deep learning models 5. The challenges of deploying models in production environments and keeping them updated"
  },
  {
    "objectID": "talks/ml-intro/index.html#clustering",
    "href": "talks/ml-intro/index.html#clustering",
    "title": "Introduction to Machine Learning",
    "section": "Clustering",
    "text": "Clustering\n\nGene expression measurements."
  },
  {
    "objectID": "talks/ml-intro/index.html#classes-of-unsupervised-learning-algorithms",
    "href": "talks/ml-intro/index.html#classes-of-unsupervised-learning-algorithms",
    "title": "Introduction to Machine Learning",
    "section": "Classes of unsupervised learning algorithms",
    "text": "Classes of unsupervised learning algorithms\n\n\n\n\n\nClustering.\n\n\n\n\n\n\nDimensionality reduction."
  },
  {
    "objectID": "talks/ml-intro/index.html#clustering-1",
    "href": "talks/ml-intro/index.html#clustering-1",
    "title": "Introduction to Machine Learning",
    "section": "Clustering",
    "text": "Clustering\n\nThinking about similarities and differences"
  },
  {
    "objectID": "talks/ml-intro/index.html#clustering-2",
    "href": "talks/ml-intro/index.html#clustering-2",
    "title": "Introduction to Machine Learning",
    "section": "Clustering",
    "text": "Clustering\n\nAlgorithm for hierarchical clustering"
  },
  {
    "objectID": "talks/ml-intro/index.html#clustering-3",
    "href": "talks/ml-intro/index.html#clustering-3",
    "title": "Introduction to Machine Learning",
    "section": "Clustering",
    "text": "Clustering\n\nAlgorithm for hierarchical clustering"
  },
  {
    "objectID": "talks/ml-intro/index.html#clustering-4",
    "href": "talks/ml-intro/index.html#clustering-4",
    "title": "Introduction to Machine Learning",
    "section": "Clustering",
    "text": "Clustering\n\nAlgorithm for hierarchical clustering"
  },
  {
    "objectID": "talks/ml-intro/index.html#dimensionality-reduction",
    "href": "talks/ml-intro/index.html#dimensionality-reduction",
    "title": "Introduction to Machine Learning",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nSchematic PCA."
  },
  {
    "objectID": "talks/ml-intro/index.html#dimensionality-reduction-1",
    "href": "talks/ml-intro/index.html#dimensionality-reduction-1",
    "title": "Introduction to Machine Learning",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nUsing dimensionality reduction to explore 22,000 dimensions of gene expression data on 280 samples.\nhttps://seandavi.github.io/ITR/geoquery_mds.html"
  },
  {
    "objectID": "talks/ml-intro/index.html#future-directions-in-machine-learning",
    "href": "talks/ml-intro/index.html#future-directions-in-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Future Directions in Machine Learning",
    "text": "Future Directions in Machine Learning\n\nAutoML: Automating the ML pipeline\nFederated Learning: Training models on decentralized data\nExplainable AI: Making black-box models more interpretable\nQuantum Machine Learning: Leveraging quantum computing for ML\nContinual Learning: Adapting to new data without forgetting old patterns"
  },
  {
    "objectID": "talks/ml-intro/index.html#conclusion",
    "href": "talks/ml-intro/index.html#conclusion",
    "title": "Introduction to Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\n\nSupervised learning: Predicting outcomes based on labeled data\n\nClassification: Assigning labels to data\nRegression: Predicting continuous values\n\nUnsupervised learning: Finding patterns in unlabeled data\n\nClustering: Grouping similar data points\nDimensionality reduction: Simplifying complex data by reducing dimensions\n\nReinforcement learning: Learning through interaction with an environment"
  },
  {
    "objectID": "talks/ml-intro/index.html#resources",
    "href": "talks/ml-intro/index.html#resources",
    "title": "Introduction to Machine Learning",
    "section": "Resources",
    "text": "Resources\n\nRecent reviews\n\nMachine Learning in Genomic Medicine: A Review of Computational Problems and Data Sets\nOpportunities And Obstacles For Deep Learning In Biology And Medicine\n\nHands-on Tutorials\n\nhttps://seandavi.github.io/RBiocBook\nMany online courses and tutorials\n\nBlogs and online materials\n\nhttps://blog.recast.ai/machine-learning-algorithms/\nhttps://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A\nhttps://www.youtube.com/c/joshstarmer\n\nMachine Learning in R\n\nhttps://www.datacamp.com/community/tutorials/machine-learning-in-r\nhttps://daviddalpiaz.github.io/r4sl/"
  },
  {
    "objectID": "talks/ml-intro/index.html#questions",
    "href": "talks/ml-intro/index.html#questions",
    "title": "Introduction to Machine Learning",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "talks/ml-intro/index.html#additional-resources",
    "href": "talks/ml-intro/index.html#additional-resources",
    "title": "Introduction to Machine Learning",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nBooks: “Introduction to Machine Learning with Python” by Müller and Guido\nOnline Courses: Andrew Ng’s Machine Learning course on Coursera\nWebsites: Towards Data Science, KDnuggets\nLibraries: scikit-learn, TensorFlow, PyTorch\n\n\nProvide these resources for students who want to dive deeper into ML. Mention that hands-on practice is crucial for truly understanding ML concepts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]