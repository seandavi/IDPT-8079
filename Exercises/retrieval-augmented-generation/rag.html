<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.19">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sean Davis">

<title>Large Language Models and Retrieval-Augmented Generation: A Technical Overview – IDPT-8079: Frontier of AI in Medicine</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-018089954d508eae8a473f0b7f0491f0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-fe4cc245a31873b96c1add286a26fa35.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-KLLV1GCF4E"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-KLLV1GCF4E', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">IDPT-8079: Frontier of AI in Medicine</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#large-language-models" id="toc-large-language-models" class="nav-link active" data-scroll-target="#large-language-models">Large Language Models</a>
  <ul class="collapse">
  <li><a href="#training-gpt-4" id="toc-training-gpt-4" class="nav-link" data-scroll-target="#training-gpt-4">Training GPT-4</a></li>
  </ul></li>
  <li><a href="#addressing-limitations-of-llm-data" id="toc-addressing-limitations-of-llm-data" class="nav-link" data-scroll-target="#addressing-limitations-of-llm-data">Addressing Limitations of LLM data</a>
  <ul class="collapse">
  <li><a href="#rag-drivers-and-benefits" id="toc-rag-drivers-and-benefits" class="nav-link" data-scroll-target="#rag-drivers-and-benefits">RAG Drivers and Benefits</a></li>
  <li><a href="#the-role-of-prompts-in-llm-interaction" id="toc-the-role-of-prompts-in-llm-interaction" class="nav-link" data-scroll-target="#the-role-of-prompts-in-llm-interaction">The Role of Prompts in LLM Interaction</a></li>
  <li><a href="#retrieval-augmented-generation-rag" id="toc-retrieval-augmented-generation-rag" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</a></li>
  <li><a href="#embeddings-and-chunking-in-rag" id="toc-embeddings-and-chunking-in-rag" class="nav-link" data-scroll-target="#embeddings-and-chunking-in-rag">Embeddings and Chunking in RAG</a></li>
  <li><a href="#embeddings-and-chunking-in-rag-1" id="toc-embeddings-and-chunking-in-rag-1" class="nav-link" data-scroll-target="#embeddings-and-chunking-in-rag-1">Embeddings and Chunking in RAG</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Large Language Models and Retrieval-Augmented Generation: A Technical Overview</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="string">Sean Davis, MD, PhD</a> <a href="mailto:seandavi@gmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-8991-6458" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://medschool.cuanschutz.edu/">
            University of Colarado Anschutz School of Medicine
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<p>Large Language Models (LLMs) have revolutionized natural language processing by generating human-like text and enabling a wide range of applications, from chatbots to language translation. However, LLMs have limitations that impact how valuable they are in a medical context.</p>
<p>Here are a few scenarios where LLMs may fall short:</p>
<ul>
<li>A healthcare provider wants to use clinical guidelines that have been recently updated, but the LLM is not aware of the latest changes.</li>
<li>A surgery planning coordinator wants to use a chatbot to assist with scheduling, but the LLM lacks access to the hospital’s scheduling system.</li>
<li>A medical school wants to provide medical students with a learning guide that is based on the patient that they admitted yesterday, including the latest research on the patient’s condition, but the LLM does not have access to the patient’s electronic health record.</li>
<li>A researcher wants to use an LLM to generate hypotheses based on the latest research, but the LLM is not trained on the most recent data such as preprints or conference abstracts.</li>
<li>A private practice wants to triage and draft responses to patient electronic messsages using an LLM, but the LLM does not have access to the practice’s electronic health record system.</li>
</ul>
<p>Here, we’ll explore how Retrieval-Augmented Generation (RAG) can address these limitations by combining the strengths of LLMs with external knowledge retrieval. We will discuss the technical aspects of RAG, including embeddings, chunking, and the retrieval process, and how these components work together to enhance the capabilities of LLMs in medical applications.</p>
<div id="fig-rag" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="20241010152039.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Retrieval augmented not in action (top) shows Claude’s response to the prompt “What is the weather today in Denver, Colorado?” Bottom shows the response from OpenAI’s ChatGPT that went out and searched the web before providing a response."><img src="20241010152039.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Retrieval augmented not in action (top) shows Claude’s response to the prompt “What is the weather today in Denver, Colorado?” Bottom shows the response from OpenAI’s ChatGPT that went out and searched the web before providing a response.
</figcaption>
</figure>
</div>
<section id="large-language-models" class="level2">
<h2 class="anchored" data-anchor-id="large-language-models">Large Language Models</h2>
<p>Large language models are machine learning models trained on vast amounts of text data to understand and generate human language (but are not necessarily intelligent <span class="citation" data-cites="noauthor_chatgpt_nodate">(<a href="#ref-noauthor_chatgpt_nodate" role="doc-biblioref"><span>“<span>ChatGPT</span> Is Not "True <span>AI</span>." <span>A</span> Computer Scientist Explains Why - <span>Big</span> <span>Think</span>”</span> n.d.</a>)</span>). These models are based on neural networks, particularly transformer architectures, which excel at learning long-range dependencies between words and concepts. By training on extensive text corpora, including medical literature, clinical notes, and general knowledge sources, LLMs can model the complex relationships between terms, diagnoses, and treatments in medical data. They are in essence very complex pattern recognition systems that can generate text that is contextually relevant and coherent. Another way of thinking of them is as incredibly powerful autocomplete systems that can generate text based on a given prompt. A third was is to think of them as “language calculators” that can perform tasks like translation, summarization, and question-answering.</p>
<section id="training-gpt-4" class="level3">
<h3 class="anchored" data-anchor-id="training-gpt-4">Training GPT-4</h3>
<p>It is thought that GPT-4 was trained on about 575 GB of data representing a diverse range of text sources, including books, articles, and websites. The training process involved optimizing the model’s parameters to predict the next word in a sentence, a task known as <strong>autoregressive language modeling</strong>. By learning the statistical patterns in the training data, GPT-4 can generate coherent and contextually relevant text in response to user prompts. The underlying model has about 1.8 trillion parameters. The training process is thought to have involved <span class="math inline">\(2.5 \cdot 10^{25}\)</span> floating-point operations, which is equivalent to running a single CPU core for about 1.5 million years. Your laptop would take a mere 100,000 years to complete the same task.</p>
<p>Despite being trained on vast amounts of data, data types or sources that are not included in the training data are not accessible to the model. This limitation can be particularly problematic in dynamic domains like healthcare, where new research, guidelines, and patient data are constantly emerging. This is where there is a need for optimization strategies like retrieval-augmented generation and fine-tuning to enhance the model’s performance in specific contexts.</p>
</section>
</section>
<section id="addressing-limitations-of-llm-data" class="level2">
<h2 class="anchored" data-anchor-id="addressing-limitations-of-llm-data">Addressing Limitations of LLM data</h2>
<p>It is worth noting that RAG is not the only approach for integrating external knowledge into LLMs (See <a href="#fig-optimization" class="quarto-xref">Figure&nbsp;2</a>). Another method is <strong>fine-tuning</strong>, which can be acomplished by training the <em>existing</em> model on new data. However, fine-tuning requires access to a large amount of labeled data, which may not always be available or feasible in practice. RAG, on the other hand, leverages external knowledge sources without modifying the model’s internal parameters, making it a more flexible and scalable solution for incorporating real-world information into LLMs.</p>
<div id="fig-optimization" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-optimization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="20241010133750.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt Engineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on the other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research progresses, Modular RAG has become more integrated with fine-tuning techniques."><img src="20241010133750.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-optimization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. <strong>Prompt Engineering</strong> requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on the other hand, involves further training the model. In the early stages of RAG (<strong>Naive RAG</strong>), there is a low demand for model modifications. As research progresses, <strong>Modular RAG</strong> has become more integrated with fine-tuning techniques.
</figcaption>
</figure>
</div>
<p>Fine-tuning involves modifying the model’s internal parameters to specialize in specific tasks or domains. This process requires high-quality labeled training data pairs and significant computational resources. In contrast, RAG augments the model’s responses with external knowledge without modifying the base model. It mainly requires reference documents or a knowledge base and is computationally less expensive than fine-tuning. RAG is particularly useful for applications that require up-to-date information, compliance with regulations, or domain-specific expertise. Table <a href="#tbl-comparison" class="quarto-xref">Table&nbsp;1</a> provides a comparison of the features and characteristics of RAG and fine-tuning for model optimization and adaptation.</p>
<div id="tbl-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: A comparison of the features and characteristics of RAG vs fine-tuning for model optimization and adaptation.
</figcaption>
<div aria-describedby="tbl-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Fine-tuning</th>
<th style="text-align: left;">RAG</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Purpose</td>
<td style="text-align: left;">Modifies base model weights to specialize in specific tasks/domains</td>
<td style="text-align: left;">Augments model responses with external knowledge without modifying the base model</td>
</tr>
<tr class="even">
<td style="text-align: left;">Training Data Requirements</td>
<td style="text-align: left;">Requires high-quality labeled training data pairs</td>
<td style="text-align: left;">Requires only reference documents/knowledge base</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Computational Cost</td>
<td style="text-align: left;">High - requires significant compute resources for training</td>
<td style="text-align: left;">Lower - mainly needs storage and embedding computation</td>
</tr>
<tr class="even">
<td style="text-align: left;">Updates &amp; Maintenance</td>
<td style="text-align: left;">Requires complete retraining to update knowledge</td>
<td style="text-align: left;">Easy to update by simply modifying the knowledge base</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Consistency</td>
<td style="text-align: left;">Generally more consistent in specialized domain</td>
<td style="text-align: left;">May vary based on retrieval quality</td>
</tr>
<tr class="even">
<td style="text-align: left;">Response Speed</td>
<td style="text-align: left;">Fast inference once trained</td>
<td style="text-align: left;">Slightly slower due to retrieval step</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Memory Requirements</td>
<td style="text-align: left;">Fixed model size</td>
<td style="text-align: left;">Scales with knowledge base size</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hallucination Risk</td>
<td style="text-align: left;">Lower for domain-specific knowledge</td>
<td style="text-align: left;">Can be lower when retrievals are accurate</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cost</td>
<td style="text-align: left;">High upfront cost for training</td>
<td style="text-align: left;">Lower upfront cost, ongoing storage/compute costs</td>
</tr>
<tr class="even">
<td style="text-align: left;">Use Cases</td>
<td style="text-align: left;">- Domain-specific applications<br>- Style transfer<br>- Consistent brand voice</td>
<td style="text-align: left;">- Question answering<br>- Up-to-date information<br>- Compliance requirements</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Flexibility</td>
<td style="text-align: left;">Less flexible - tied to training data</td>
<td style="text-align: left;">More flexible - easy to modify knowledge</td>
</tr>
<tr class="even">
<td style="text-align: left;">Development Time</td>
<td style="text-align: left;">Longer - requires careful training and validation</td>
<td style="text-align: left;">Shorter - focus on knowledge engineering</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>We won’t go into the details of fine-tuning here since it is both costly and inconvenient for everyday use. Instead, we will focus on RAG, which is more flexible and cost-effective for augmenting LLMs with external knowledge.</p>
<section id="rag-drivers-and-benefits" class="level3">
<h3 class="anchored" data-anchor-id="rag-drivers-and-benefits">RAG Drivers and Benefits</h3>
<p>RAG addresses several limitations of LLMs by integrating external knowledge retrieval into the generation process. Table <a href="#tbl-limitations" class="quarto-xref">Table&nbsp;2</a> outlines the key areas where LLMs fall short and how RAG can mitigate these limitations.</p>
<div id="tbl-limitations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Areas where LLMs fall short and how RAG can address these limitations.
</figcaption>
<div aria-describedby="tbl-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 71%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Driver</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Knowledge Freshness</td>
<td style="text-align: left;">• LLMs have a knowledge cutoff date from their training<br>• RAG allows access to current information by retrieving up-to-date documents<br>• Particularly important for rapidly changing domains like tech, news, and business</td>
</tr>
<tr class="even">
<td style="text-align: left;">Accuracy &amp; Hallucination Reduction</td>
<td style="text-align: left;">• LLMs can sometimes fabricate or “hallucinate” information<br>• RAG grounds responses in specific source documents<br>• Provides citations and references to validate information<br>• Reduces false or inaccurate statements</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Domain-Specific Knowledge</td>
<td style="text-align: left;">• LLMs have broad but sometimes shallow knowledge<br>• RAG enables deep expertise in specific domains by accessing:<br>&nbsp;&nbsp;- Internal company documents<br>&nbsp;&nbsp;- Industry-specific materials<br>&nbsp;&nbsp;- Technical documentation<br>&nbsp;&nbsp;- Specialized research papers</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data Privacy &amp; Security</td>
<td style="text-align: left;">• Base LLMs don’t have access to private/proprietary information<br>• RAG allows secure access to private documents while maintaining:<br>&nbsp;&nbsp;- Data sovereignty<br>&nbsp;&nbsp;- Compliance requirements<br>&nbsp;&nbsp;- Confidentiality</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cost Efficiency</td>
<td style="text-align: left;">• Fine-tuning LLMs is expensive and resource-intensive<br>• RAG provides a more cost-effective way to augment LLM capabilities<br>• Allows dynamic updates without retraining</td>
</tr>
<tr class="even">
<td style="text-align: left;">Verifiability</td>
<td style="text-align: left;">• RAG enables tracing responses back to source documents<br>• Important for:<br>&nbsp;&nbsp;- Compliance requirements<br>&nbsp;&nbsp;- Audit trails<br>&nbsp;&nbsp;- Quality assurance<br>&nbsp;&nbsp;- Building trust in AI systems</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Customization</td>
<td style="text-align: left;">• Organizations can tailor responses based on their specific knowledge base<br>• Enables consistent messaging and brand voice<br>• Allows for context-aware responses</td>
</tr>
<tr class="even">
<td style="text-align: left;">Real-time Information Access</td>
<td style="text-align: left;">• RAG can connect to live databases or document stores<br>• Enables responses based on current data states<br>• Useful for dynamic information like inventory or pricing</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>One of the most fundamental limitations and dangers of LLMs is their propensity to generate incorrect or fabricated information, known as <strong>hallucination</strong> <span class="citation" data-cites="zhang2023sirenssongaiocean">(<a href="#ref-zhang2023sirenssongaiocean" role="doc-biblioref">Zhang et al. 2023</a>)</span>. This occurs because LLMs, when faced with incomplete or ambiguous inputs, will still generate outputs that may appear coherent but are not grounded in factual data. In a medical context, this could manifest as the AI confidently suggesting a treatment that is not supported by clinical evidence or misinterpreting symptoms due to the limitations of its training data. For example, if the AI is asked about a rare disease that was underrepresented in its training set, it may generate plausible but inaccurate diagnostic recommendations. Understanding this limitation is critical in fields where the cost of misinformation is high, such as healthcare. RAG can help mitigate this risk by providing the model with accurate, up-to-date information from external sources, reducing the likelihood of hallucination and improving the quality of generated responses.</p>
</section>
<section id="the-role-of-prompts-in-llm-interaction" class="level3">
<h3 class="anchored" data-anchor-id="the-role-of-prompts-in-llm-interaction">The Role of Prompts in LLM Interaction</h3>
<p>A <strong>prompt</strong> is the input provided to the model, serving as the basis for generating a response. In our example, the medical professional’s description of symptoms, medical history, and any specific questions about diagnoses or treatments form the user prompt. There are several types of prompts:</p>
<ul>
<li><strong>System prompts</strong>: These set the stage for how the model should behave. For example, a system prompt may instruct the model to maintain a formal, professional tone or limit responses to a concise format appropriate for medical decision-making.</li>
<li><strong>User prompts</strong>: These are the direct inputs from the user, such as “What are the likely causes of chest pain in a 50-year-old patient with a history of smoking?”</li>
<li><strong>Additional context</strong>: This refers to background information or structured data that may inform the model’s response, such as the patient’s prior diagnoses, lab results, or family history.</li>
</ul>
<p>With commercially available LLMs, we can supply quite a bit of context to the model. For example, we can often drop a PDF into the model to provide additional context. This works well when the context is relatively small and can be easily processed by the model.</p>
<p>However, when the context is large (e.g., multiple books or all patient records for patients in a health system) or would require accessing external systems such as a database, we can utilize retrieval-augmented generation (RAG) to provide the model with the necessary information to generate a response. The RAG approach combines the strengths of LLMs with external knowledge retrieval to enhance the model’s performance in complex, information-rich domains like healthcare.</p>
</section>
<section id="retrieval-augmented-generation-rag" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</h3>
<div id="fig-abc" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-abc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="20241009194727.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and generation; it includes methods such as iterative and adaptive retrieval."><img src="20241009194727.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-abc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and generation; it includes methods such as iterative and adaptive retrieval.
</figcaption>
</figure>
</div>
<p>To address the limitations of a model’s built-in knowledge and context window, <strong>retrieval-augmented generation (RAG)</strong> integrates external knowledge retrieval into the generation process (See <a href="#fig-abc" class="quarto-xref">Figure&nbsp;3</a>). In our motivating example, if the medical professional inputs symptoms related to a rare disease, a RAG-enabled system would not rely solely on the LLM’s internal parameters but would retrieve relevant documents—such as the latest clinical guidelines or research papers on the disease—from an external knowledge base.</p>
<p>RAG systems typically work in two phases:</p>
<ol type="1">
<li><p><strong>Retrieval</strong>: The model searches a database or corpus of external documents (such as medical research papers or guidelines) for relevant information based on the input.</p></li>
<li><p><strong>Generation</strong>: The retrieved documents are used to inform the model’s response, augmenting the language generation process with precise and up-to-date information.</p></li>
</ol>
<p>This approach significantly reduces the risk of hallucination by grounding responses in verified data. In our use case, RAG would enable the AI to search clinical databases for information about rare diseases, ensuring that its diagnostic suggestions are based on real-world evidence, not just patterns in its training data.</p>
</section>
<section id="embeddings-and-chunking-in-rag" class="level3">
<h3 class="anchored" data-anchor-id="embeddings-and-chunking-in-rag">Embeddings and Chunking in RAG</h3>
<p>While the most common implementation of RAG involves a two-step process of retrieval followed by generation, the technical details can vary based on the specific use case and system architecture. <a href="#tbl-retrieval-approaches" class="quarto-xref">Table&nbsp;3</a> provides an overview of different retrieval approaches for RAG systems, highlighting their best use cases and considerations.</p>
<div id="tbl-retrieval-approaches" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-retrieval-approaches-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: A comparison of different retrieval approaches for RAG systems, including their best use cases and considerations.
</figcaption>
<div aria-describedby="tbl-retrieval-approaches-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 26%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Retrieval Approach</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Best Used For</th>
<th style="text-align: left;">Considerations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Vector Databases</td>
<td style="text-align: left;">• Store document embeddings for semantic search<br>• Examples: Pinecone, Weaviate, Milvus<br>• Efficient similarity search</td>
<td style="text-align: left;">• Unstructured documents<br>• Long-form content<br>• Knowledge bases<br>• Documentation</td>
<td style="text-align: left;">• Embedding quality matters<br>• Chunking strategy important<br>• Storage costs for large collections</td>
</tr>
<tr class="even">
<td>SQL Databases</td>
<td style="text-align: left;">• Direct querying of structured data<br>• Can generate SQL from natural language<br>• Precise, structured retrieval</td>
<td style="text-align: left;">• Transactional data<br>• Product catalogs<br>• Customer records<br>• Financial data</td>
<td style="text-align: left;">• Schema design crucial<br>• Query generation accuracy<br>• Performance with complex joins</td>
</tr>
<tr class="odd">
<td>Web Search APIs</td>
<td style="text-align: left;">• Real-time internet search<br>• APIs like Google Custom Search<br>• Bing Web Search API</td>
<td style="text-align: left;">• Current events<br>• Public information<br>• Market research<br>• Competitor analysis</td>
<td style="text-align: left;">• Cost per query<br>• Rate limits<br>• Quality of results<br>• Data freshness</td>
</tr>
<tr class="even">
<td>Domain-Specific APIs</td>
<td style="text-align: left;">• Weather APIs<br>• Stock market data<br>• Sports scores<br>• Geographic data</td>
<td style="text-align: left;">• Real-time data needs<br>• Specialized information<br>• Dynamic content</td>
<td style="text-align: left;">• API reliability<br>• Integration complexity<br>• Costs and rate limits</td>
</tr>
<tr class="odd">
<td>Code Repositories</td>
<td style="text-align: left;">• GitHub API<br>• Source code indexing<br>• Documentation sites</td>
<td style="text-align: left;">• Technical support<br>• Code examples<br>• API usage<br>• Debugging</td>
<td style="text-align: left;">• Code parsing complexity<br>• Version management<br>• Context understanding</td>
</tr>
<tr class="even">
<td>Graph Databases</td>
<td style="text-align: left;">• Neo4j, Amazon Neptune<br>• Relationship-based retrieval<br>• Knowledge graphs</td>
<td style="text-align: left;">• Complex relationships<br>• Network analysis<br>• Dependencies</td>
<td style="text-align: left;">• Graph query complexity<br>• Maintenance overhead<br>• Schema design</td>
</tr>
<tr class="odd">
<td>Hybrid Search</td>
<td style="text-align: left;">• Combines multiple approaches<br>• Example: keyword + semantic search<br>• Multi-index retrieval</td>
<td style="text-align: left;">• Complex queries<br>• Diverse content types<br>• High accuracy needs</td>
<td style="text-align: left;">• Orchestration complexity<br>• Result ranking<br>• Performance overhead</td>
</tr>
<tr class="even">
<td>File Systems</td>
<td style="text-align: left;">• Local document storage<br>• Network drives<br>• Document management systems</td>
<td style="text-align: left;">• Internal documents<br>• Legacy systems<br>• Offline access</td>
<td style="text-align: left;">• File format handling<br>• Access permissions<br>• Indexing overhead</td>
</tr>
<tr class="odd">
<td>Cache Layers</td>
<td style="text-align: left;">• Redis, Memcached<br>• Frequently accessed data<br>• Results caching</td>
<td style="text-align: left;">• High-performance needs<br>• Repeated queries<br>• Cost optimization</td>
<td style="text-align: left;">• Cache invalidation<br>• Storage limits<br>• Consistency management</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>When thinking about applying RAG to a specific use case, it is essential to consider the retrieval approach that best suits the data sources and information needs. For example, a healthcare AI system may benefit from using vector databases to store document embeddings for semantic search, enabling efficient retrieval of relevant medical literature based on input symptoms. On the other hand, a financial advisory chatbot could leverage SQL databases to query structured data on stock prices and market trends in real time. A private practice clinic might use domain-specific APIs to access patient appointment schedules and medical records securely.</p>
</section>
<section id="embeddings-and-chunking-in-rag-1" class="level3">
<h3 class="anchored" data-anchor-id="embeddings-and-chunking-in-rag-1">Embeddings and Chunking in RAG</h3>
<p>Two critical components of RAG are <strong>embeddings</strong> and <strong>chunking</strong>.</p>
<ul>
<li><p><strong>Embeddings</strong>: Embeddings are vector representations of text that capture the semantic meaning of words and phrases. In the retrieval step, the input (e.g., symptoms or a diagnostic question) is transformed into an embedding, which is then used to search the knowledge base for relevant information. Embeddings ensure that even if the input uses different terminology, the retrieval process can still match it with relevant documents that use similar meanings.</p>
<p>In our example, the AI system would convert the input symptoms into an embedding and use it to search for medical documents that discuss those symptoms, even if the wording differs slightly (e.g., “myocardial infarction” versus “heart attack”).</p></li>
<li><p><strong>Chunking</strong>: Large documents, such as clinical guidelines or research papers, are often too extensive to process in their entirety. <strong>Chunking</strong> breaks these documents into smaller, manageable sections, allowing the model to retrieve specific parts relevant to the input. When a document on cardiovascular health is divided into chunks, the model can retrieve only the sections related to smoking and chest pain, rather than processing the entire document.</p></li>
</ul>
<p>These techniques allow the AI system to quickly access relevant information without being overwhelmed by the volume of text.</p>
</section>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<ul>
<li>Give <a href="https://notebooklm.google/">google notebookLM</a> a try.</li>
<li>Use notebookLM to create and interact with a notebook about retrieval augmented generation. Include resources such as papers, blog posts, chatgpt responses (text), videos.</li>
</ul>
</section>
<section id="references" class="level2">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-noauthor_chatgpt_nodate" class="csl-entry" role="listitem">
<span>“<span>ChatGPT</span> Is Not "True <span>AI</span>." <span>A</span> Computer Scientist Explains Why - <span>Big</span> <span>Think</span>.”</span> n.d. Accessed October 10, 2024. <a href="https://bigthink.com/the-future/artificial-general-intelligence-true-ai/">https://bigthink.com/the-future/artificial-general-intelligence-true-ai/</a>.
</div>
<div id="ref-zhang2023sirenssongaiocean" class="csl-entry" role="listitem">
Zhang, Yue, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, et al. 2023. <span>“Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.”</span> <a href="https://arxiv.org/abs/2309.01219">https://arxiv.org/abs/2309.01219</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{davis,
  author = {Davis, Sean},
  title = {Large {Language} {Models} and {Retrieval-Augmented}
    {Generation:} {A} {Technical} {Overview}},
  url = {https://seandavi.github.io/IDPT-8079/Exercises/retrieval-augmented-generation/rag.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-davis" class="csl-entry quarto-appendix-citeas" role="listitem">
Davis, Sean. n.d. <span>“Large Language Models and Retrieval-Augmented
Generation: A Technical Overview.”</span> <a href="https://seandavi.github.io/IDPT-8079/Exercises/retrieval-augmented-generation/rag.html">https://seandavi.github.io/IDPT-8079/Exercises/retrieval-augmented-generation/rag.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/seandavi\.github\.io\/IDPT-8079\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","selector":".lightbox","openEffect":"zoom","descPosition":"bottom","loop":false});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>